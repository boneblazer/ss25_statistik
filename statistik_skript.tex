\documentclass[a4paper,11pt]{article}

\usepackage{babel}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{ngerman}
\usepackage{verbatim}
\usepackage{mathtools}
\usepackage{makecell}
\usepackage{hyperref}
\usepackage{tikz}


%margins
\addtolength{\oddsidemargin}{-0.75in}
\addtolength{\evensidemargin}{-0.75in}
\addtolength{\textwidth}{1in}
\addtolength{\topmargin}{-.5in}
\addtolength{\textheight}{1.0in}


%annotation above equals
\newcommand\eqthreei{\overset{\mathclap{\scriptscriptstyle (iii)}}{=}}
\newcommand\eqoneone{\overset{\mathclap{\scriptscriptstyle (1.1)}}{=}}
\newcommand\eqonetwo{\overset{\mathclap{\scriptscriptstyle (1.2)}}{=}}
\newcommand\eqonethree{\overset{\mathclap{\scriptscriptstyle (1.3)}}{=}}
\newcommand\eqonefive{\overset{\mathclap{\scriptscriptstyle (1.5)}}{=}}
\newcommand\eqonei{\overset{\mathclap{\scriptscriptstyle (i)}}{=}}
\newcommand\eqasb{\overset{\mathclap{\scriptscriptstyle A \subset B}}{=}}
\newcommand\geqonei{\overset{\mathclap{\scriptscriptstyle (i)}}{\geq}}
\newcommand\Raonethree{\overset{\mathclap{\scriptscriptstyle (1.3)}}{\Rightarrow}}
\newcommand\stw{\overset{\mathclap{\scriptscriptstyle (2.1.2)}}{=}}
\newcommand\Btuwon{\overset{\mathclap{\scriptscriptstyle (B.2.1)}}{=}}
\newcommand\Dtuwon{\overset{\mathclap{\scriptscriptstyle (D.2.1)}}{=}}
\newcommand\fover{\overset{\mathclap{\scriptscriptstyle (f)}}{=}}

\newcommand\eqsigmaadd{\overset{\mathclap{\scriptscriptstyle (\sigma\text{-Add.})}}{=}}
\newcommand\eqthreetwog{\overset{\mathclap{\scriptscriptstyle (Satz 3.2(g))}}{=}}
\newcommand\sfotwoa{\overset{\mathclap{\scriptscriptstyle Satz \hspace{2pt} 4.2 \hspace{2pt} (a)}}{=}}
\newcommand\impthreetwog{\overset{\mathclap{\scriptscriptstyle (Satz 3.2(g))}}{\Rightarrow}}
\newcommand\sfothree{\overset{\mathclap{\scriptscriptstyle Satz \hspace{2pt} 4.3 \hspace{2pt} mit \hspace{2pt} g(x)=x^2}}{=}}
\newcommand\sfofoa{\overset{\mathclap{\scriptscriptstyle Satz \hspace{2pt} 4.4 \hspace{2pt} (a)}}{=}}
\newcommand\bspfoonc{\overset{\mathclap{\scriptscriptstyle Bsp. \hspace{2pt} 4.1 \hspace{2pt} (c)}}{=}}
\newcommand\sfofoe{\overset{\mathclap{\scriptscriptstyle Satz \hspace{2pt} 4.4 \hspace{2pt} (e)}}{=}}
\newcommand\sfofoc{\overset{\mathclap{\scriptscriptstyle Satz \hspace{2pt} 4.4 \hspace{2pt} (c)}}{=}}
\newcommand\jekmo{\underset{\mathclap{\scriptscriptstyle j=k-1}}{=}}





%unused
%Falls die Ereignisse $A_1,A_2,\dots$ eine Zerlegung von $\Omega$ bilden, d.h. falls gilt $A_1,A_2,\dots$ sind paarweise disjunkt und $A_1 \cup A_2 \cup \dots = \Omega$, 
%dann gilt für jedes Ereignis B mit $P(B) = P(B\cap A_1) + P(B\cap A_2) + \dots$, denn: 
%$P(B)=P(B\cap\Omega)=P(B\cap(A_1\cup A_2\cup\dots))=P((B\cap A_1)\cup(B\cap A_2)\cup\dots)$ (es gilt: $(B\cap A_i)$ und $(B\cap A_j)$ sind paarweise disjunkt für $i\neq j$.) 



\begin{document}

\title{\Huge{\textbf{Statistik - SoSe 2025}}}
\author{}
\date{}
\maketitle
\tableofcontents

\part{Wahrscheinlichkeitsrechnung}

\section{Grundbegriffe}

\subsection{Ergebnismenge und Ereignisse}
Alle möglichen Ergebnisse eines Zufallsexperiments fasst man zur Ergebismenge $\Omega$ (Groß Omega) zusammen.

\vspace{6pt}
\noindent\textbf{Bsp. 1.1:} Zweimaliges Werfen eines Würfels
\begin{align*} 
    \Omega &= \{(1,1),(1,2),\dots,(1,6),(2,1),\dots,(6,6)\} \\
        &= \{(i,j): 1\leq i\leq 6,1\leq j\leq 6\}\\
        &=\{1,\dots,6\}\times\{1,\dots,6\}=\{1,\dots,6\}^2
\end{align*}

\vspace{6pt}
\noindent\textbf{Bsp. 1.2:} Ziehung der Lottozahlen (6 aus 49)
\[\Omega = \{(\omega_1,\dots,\omega_6):\omega_i \in \{1,2,\dots,49\}, 1\leq i\leq6, \omega_1 < \omega_2 <\dots<\omega_6\}\]

\vspace{6pt}
\noindent\textbf{Bsp. 1.3:} Verkaufszahlen. Ein Laden erhält morgens 3 Tageszeitungen $Z_1,Z_2,Z_3$, und zwar 100 bzw. 200 bzw. 250 Stück. Die verkauften Anzahlen sind dann als Ergebis eines Zufallsexperiments zu interpretieren.
\begin{align*}
    \Omega &= \{(\omega_1,\omega_2,\omega_3): \omega_1 \in \{0,1,\dots,100\}, \omega_2 \in \{0,1,\dots,200\}, \omega_3 \in \{0,1,\dots,250\}\} \\
    &= \{0,1,\dots,100\}\times\{0,1,\dots,200\}\times\{0,1,\dots,250\}
\end{align*}

\vspace{6pt}
\noindent Ein Ereignis ist eine Teilmenge der Ergebnismenge. Ereignisse werden zunächst verbal beschrieben 
und lassen sich dann als Teilmengen von $\Omega$ auffassen.

\vspace{6pt}
\noindent\textit{In Bsp. 1.1:}
\begin{itemize}
    \item[-] Ereignis E: Augensumme ist 10; zugeordnete Teilmenge von $\Omega: E = \{(4,6),(5,5),(6,4)\}$
    \item[-] E: Pasch; $E = \{(1,1),(2,2),(3,3),(4,4),(5,5),(6,6)\}$
    \item[-] E: mindestens eine 6; $E = \{(6,1),(6,2),\dots(6,6),(5,6),\dots,(1,6)\}$
\end{itemize}

\vspace{6pt}
\noindent\textit{In Bsp. 1.3:}
\newline Ereignis A: Von jeder der 3 Zeitungen werden mindestens 50 Stück verkauft
\newline Teilmenge: $A = \{(\omega_1,\omega_2,\omega_3)\in\Omega:\omega_1\leq50,\omega_2\leq50,\omega_3\leq50\}=\{50,51,\dots,100\}\times\{50,51,\dots,200\}\times\{50,51,\dots,250\}$

\vspace{6pt}
\noindent Ist $\omega\in\Omega$ das betrachtete Ergebnis eines Zufallsexperiments und ist $E\subset\Omega$ ein Ereignis, so sagt man \textit{E ist eingetreten}, falls $\omega\in E$ und \textit{E ist nicht eingetreten}, falls $\omega\notin E$.

\noindent Übertrage Begriffe der Mengenlehre auf das zufällige Eintreten von Ereignissen. (Dies wird durch die Zuordnung \textit{Ereignis} $\Leftrightarrow$ \textit{Teilmenge von $\Omega$} möglich.)

\vspace{6pt}
\begin{center}
\noindent\begin{tabular}{|c|c|}
\hline
\textbf{Mengenschreibweise} & \textbf{Interpretation für Ereignisse}\\
\hline
$A=\Omega$ & \makecell{A ist ein sicheres Ereignis \\ \small{(A tritt sicher ein)}}\\
\hline
$A=\emptyset$ & \makecell{A ist ein unmögliches Ereignis \\ \small{(A tritt nicht ein)}}\\
\hline
\makecell{$A^c = \{\omega\in\Omega, \omega\notin A\}$ \\ $\bar{A}=A^c$ \footnotesize{(andere Schreibweise)}}  & \makecell{Komplementärereignis zu A \\ \small{(A tritt nicht ein)}}\\
\hline
$A\cap B$  & \makecell{A geschnitten B. \\ \small{(A und B treten ein.)}} \\
\hline
$\bigcap_{i=1}^{n}A_i = A_1\cap\dots\cap A_n$ & \makecell{Durchschnitt der Ereignisse $A_1,\dots,A_n$ \\ \small{(Jedes dieser Ereignisse $A_1,\dots,A_n$ tritt ein.)}}\\
\hline
$A\cap B = \emptyset$ & \makecell{A und B sind disjunkt. \\ \small{(A und B treten sicher nicht zusammen ein.)}} \\
\hline
$A\cup B$ & \makecell{Vereinigung von A und B. \\ \small{(A oder B tritt ein. \glqq Oder\grqq\  ist nicht ausschließend.)}} \\
\hline
$\bigcup_{i=1}^{n}A_i = A_1\cup\dots\cup A_n$ & Mindestens eines der Ereignisse tritt ein. \\
\hline
$A\subset B$ & \makecell{Implikation: \\ \small{Aus dem Eintreten von A folgt das Eintreten von B.}} \\
\hline
$A\backslash B = A\cap B^c$ & A tritt ein, aber B tritt nicht ein. \\
\hline
\end{tabular}
\end{center}

\vspace{2pt}
\noindent Einelementige Teilmengen $\{\omega\}\subset\Omega$ (die nur einen Punkt enthalten) nennt man Elementarereignisse.

\vspace{6pt}
\noindent\textbf{Bsp. 1.4:} Einmaliges Werfen eines Würfels; $\Omega = \{1,\dots,6\}$
\newline Ereignis $A$: \glqq Augenzahl ist gerade\grqq\ $\Rightarrow$ $A=\{2,4,6\}$, Ereignis $B$: \glqq Augenzahl ist mindestens 4\grqq\ $\Rightarrow B=\{4,5,6\}$
\newline\noindent\begin{tabular}{c|c}
Schreibweise & Interpretation\\
\hline
$A^c = \{1,3,5\}$ & Augenzahl ist ungerade\\
$B^c = \{1,2,3\}$ & Augenzahl ist kleiner als 4\\
$A\cap B = \{4,6\}$ & Augenzahl ist gerade und mind. 4\\
$A\cup B = \{2,4,5,6\}$ & Augenzahl ist gerade oder mind. 4\\
$B\backslash A = B\cap A^c = \{4,5,6\}\cap\{1,3,5\}=\{5\}$ & Augenzahl ist mind. 4, aber nicht gerade.\\
$A\backslash B = A\cap B^c = \{1,3,5\}\cap\{4,5,6\}=\{1,3\}$ & Augenzahl ist nicht gerade, aber mind. 4\\
\end{tabular}

\subsubsection{Rechenregeln für Mengen}
\noindent Für beliebige Mengen A,B,C gilt:
\begin{itemize}
    \item Kommutativgesetze: $A\cup B = B\cup A$, $A\cap B = B\cap A$
    \item Assoziativgesetze: $A\cup(B\cup C)=(A\cup B)\cup C$; $A\cap(B\cap C)=(A\cap B)\cap C$
    \item Distributivgesetze: $A\cup(B\cap C) = (A\cap B) \cup (A\cap C); (A\cap B)\cup C = (A\cup C)\cap (B\cup C)$
    \item Komplement: $(A^c)^c = A$
    \item De Morgansche Regeln: $(A\cup B)^c = A^c\cap B^c, (A\cap B)^c = A^c\cup B^c$
\end{itemize}
Hinweis: Um das Komplement verwenden zu können benötigt man eine bestimmte Obermenge.

\vspace{6pt}
\noindent Allgemeiner: Für beliebige $A_1,\dots,A_n$ und B gilt:
\begin{itemize}
    \item Kommutativ- und Assoziativgesetze gelten, da die Reihenfolge keine Rolle spielt.
    \item $(\bigcup_{i=1}^{n}A_i)\cap B = \bigcup_{i=1}^{n}(A_i\cap B)$
    \item $(\bigcap_{i=1}^{n}A_i)\cup B = \bigcap_{i=1}^{n}(A_i\cup B)$
    \item $(\bigcup_{i=1}^{n}A_i)^c = \bigcap_{i=1}^{n}A_i^c$,$(\bigcap_{i=1}^{n}A_i)^c = \bigcup_{i=1}^{n}A_i^c$
    \item Für beliebige Mengen A,B gilt $A\subset B \Leftrightarrow A^c\supset B^c$
\end{itemize}

\vspace{6pt}
\noindent Beweis für $(\bigcup_{i=1}^{n}A_i)^c = \bigcap_{i=1}^{n}A_i^c$, wobei die Komplemente bzgl. $\Omega$ gebildet werden.
\newline Für jedes $\omega\in\Omega$ gilt:
\begin{align*}
\omega\in(\bigcup_{i=1}^{n}A_i)^c &\Leftrightarrow \omega\notin\bigcup_{i=1}^{n}A_i\\
&\Leftrightarrow \omega\text{ ist in keiner der Mengen } A_1,\dots,A_n\text{ enthalten.}\\ 
&\Leftrightarrow \omega\in A_1^c\ \text{und}\ \omega\in A_2^c\ \text{und}\dots\text{und}\ \omega\in A_n^c\\
&\Leftrightarrow \omega\in\bigcap_{i=1}^{n}A_i^c\\
\end{align*}

\subsection{Wahrscheinlichkeitsmaße}

\noindent\textbf{Def. 1.1:} Es sei $\Omega\neq\emptyset$ einer Ergebnismenge. 
Eine für alle Ereignisse $A\subset\Omega$ definierte reelwertige Funktion P heißt \textbf{Wahrscheinlichkeitsmaß} auf $\Omega$ falls sie die folgenden Axiome der Wahrscheinlichkeitsrechnung erfüllt:
\begin{itemize}
    \item[(i)] $P(A)\geq0$ für alle Ereignisse $A<\Omega$
    \item[(ii)] $P(\Omega)=1$
    \item[(iii)] $\sigma$-Additivität: $P(A_1 \cup A_2\cup\dots) = P(A_1)+P(A_2)+\dots$ gilt für jede endliche oder unendliche Folge von paarweise disjunkten Ereignissen $A_1,A_2,\dots\subset\Omega$    
\end{itemize}

\noindent P(A) heißt \textit{Wahrscheinlichkeit} von A (probability). 
($\Omega,P$) heißt \textit{Wahrscheinlichkeitsmodell} für das Zufallsexperiment. 
(i)-(iii) heißen \textit{Axiome der Wahrscheinlichkeitsrechnung} oder \textit{Kolmogorov-Axiome}.

\vspace{6pt}
\noindent\textbf{Bsp. 1.5:} Zweimaliges Werfen eines Würfels.
\newline $\Omega = \{(i,j):1\leq i\leq6,1\leq j\leq6\}$
\newline Für $A\subset\Omega$ sei $P(A)=\frac{|A|}{|\Omega|}=\frac{|A|}{36}$, wobei $|A|$ = Anzahl der Elemente von A. Dann ist P ein W-Maß, denn P ist reelwertig
\newline (i) $P(A) \geq0$ für alle $A\subset\Omega$
\newline (ii) $P(\Omega)=\frac{|\Omega|}{|\Omega|}=1$
\newline (iii) Sind $A_1,\dots,A_n$ paarweise disjunkte Teilmengen von $\Omega$, so ist $|\bigcup_{i=1}^{n}A_i| = \sum_{i_1}^{n}|A_n|$, 
also $P(\bigcup_{i=1}^{n}A_i) = \frac{|\bigcup_{i=1}^{n}A_i|}{|\Omega|} = \frac{\sum_{i_1}^{n}|A_n|}{|\Omega|} = \sum_{i=1}^{n}\frac{|A_i|}{|\Omega|} = \sum_{i=1}^{n}P(A_i)$.

\vspace{2pt}
\noindent Für jede unendliche Folge von paarweise disjunkten Teilmengen $A_1,\dots,A_n$ von $\Omega$ existiert ein $N\in\mathbb{N}$, so dass $A_i=\emptyset$ für alle $i>N$, da $\Omega$ endlich ist.
Es gilt also:
\newline $\Rightarrow P(\bigcup_{i=1}^{\infty}A_i)=P(\bigcup_{i=1}^{\mathbb{N}}A_i)$
\newline $\Rightarrow P(\sum_{i=1}^{\mathbb{N}}A_i)=P(\sum_{i=1}^{\infty}A_i)$
\newline $P(A_i)=P(\emptyset)=\frac{|\emptyset|}{|\Omega|}=0\ \text{für alle}\ i\notin\mathbb{N}$
\newline Für das Ergebnis A: \glqq Augensumme ist 10\grqq, also $A=\{(4,6),(5,5),(6,4)\}$ ergibt sich $P(A_i)=\frac{|A|}{|Omega|}=\frac{3}{36}=\frac{1}{12}$.
\newline Für das Ergebnis B: \glqq Pasch\grqq, also $B=\{(1,1),(2,2),\dots,(6,6)\}$ ergibt sich 
\newline$P(A)=\frac{|A|}{|Omega|}=\frac{6}{36}=\frac{1}{6}$.


\vspace{9pt}
\noindent Ist allgemeiner $\Omega \neq \emptyset$ eine beliebige endliche Menge und haben alle Ergebnisse 
(genauer: alle Elementarereignisse) dieselbe Wahrscheinlichkeit, dann folgt aus Axiomen (ii) und (iii)

\[ P(\{w\})=\frac{1}{|\Omega|} \]

\noindent und daher wegen Axiom (iii) \[ P(A)=\frac{|A|}{|\Omega|} \]

\noindent Also \[ P(A)=\frac{\text{Anzahl der Ergebnisse in A}}{\text{Anzahl aller m"oglichen Ergebnisse}} \] 

\noindent P(A) heißt \textit{Laplace-Wahrscheinlichkeit} von A.

\vspace{6pt}
\noindent\textbf{Bsp. 1.6} Roulette.
Wenn $\Omega = \{0,1,...,36\} $ ist die W'keit, dass eine ungerade Zahl fällt, $P(\{1,3,5,...,35\})=\frac{|\{1,3,5,...,35\}|}{|\Omega|} = \frac{18}{37}$.

\noindent\ Grundlegendes Prinzip des Zählens ... was heißt das?

\vspace{6pt}
\noindent\textbf{Multiplikationsregel:} Betrachte k Aufgaben. Nimm an, die 
1. Aufgabe kann auf $n_1$ Arten erledigt werden; danach kann die 
2. Aufgabe auf $n_2$ Arten erledigt werden;...; danach kann die 
k-te Aufgabe erledigt werden. (Annahme: $n_1,n_2,...,k$ sind unabhängig.) 
\newline Die Anzahl der Möglichkeiten, alle k Aufgaben zu erledigen, ist $n_1*n_2*...*k$

\vspace{6pt}
\noindent\textbf{Bsp. 1.7:} Eine faire Münze wird n-mal geworfen. 
Ergebnisse sind Folgen von Kopf(K) und Zahl(Z) der Länge n: $\Omega = \{(x_1,...,x_n): x_i\in\{K,Z\}, i = 1,...,n\}$. 
Verwende Laplace-Wahrscheinlichkeiten: $P(A)=\frac{|A|}{|\Omega|}=\frac{|A|}{2^n}$ für alle $A\in\Omega$.
Wie groß ist die Wahrscheinlichkeit von $A_j$: \glqq K fällt im j-ten Wurf zum 1. Mal\grqq?
\[A_j = \{x_1,...,x_n\}: x_i=Z \text{ für } 1 \leq i \leq j-1; x_j=K; x_i \{K,Z\} \text{ für } j+1 \leq i \leq n\]
\[A_j = 1^{j-1}\cdot 1\cdot 2^{n-j}=2^{n-j} \rightarrow P(A_j) = \frac{|A_j|}{|\Omega|}= \frac{2^{n-j}}{2^{n}}\]

\noindent\textbf{Bsp. 1.8:} Geburtstagsproblem.
W'keit, dass von k Personen 2 oder mehr am selben Tag im Jahr Geburtstag haben?
$2 \leq k \leq 365$ ohne (29.2) und verwende Laplace-W'keiten mit $\Omega=\{x_1,...,x_k\}: x_i \in \{1,...,365\}, i=1,...,k$, also 
$P(A)=\frac{|A|}{|\Omega|}=\frac{|A|}{365}$ für alle $A\subset\Omega$.
$\text{Sei }B=\{(x_1,...,x_k) \in\Omega: \text{mind. 2 der } x_i \text{ stimmen überein}\}$
\begin{align*}
    |B^c| &= \{(x_1, \dots ,x_k) \in \Omega: x_i \neq x_j \text{ für }i \neq j\} \\
    &= 365 \cdot 364 \cdot 363 \cdot\dots\cdot (365-k+1) \\
    &= \prod_{i=0}^{k-1}(365-i) \\
    \rightarrow P_k = P(B) &= \frac{|\Omega| - |B^c|}{|\Omega|} = 1 - \frac{1}{365}\cdot \prod_{i=0}^{k-1}(365-i) \\
    &= 1 - \prod_{i=0}^{k-1}(1-\frac{i}{365}) \text{           z.B. } P_{30} = 0,705
\end{align*}

\subsubsection{Rechenregeln für Wahrscheinlichkeitsmaße}

\begin{itemize}
    \item[1.1] $P(A^c)=1-P(A)$, denn $1=P(\Omega)=P(A\cup A^c) \eqthreei P(A) + P(A^c) \Rightarrow (1.1)$
    \item[1.2] $P(\emptyset)=0$, denn $P(\emptyset)=P(\Omega^c) \eqoneone 1-P(\Omega) = 1-1 = 0$
    \item[1.3] Falls $A\subset B$, dann $P(B \backslash A)=P(B)-P(A)$, denn: Falls $A\subset B$, dann $B=B\cap (A\cup A^c) = (B\cap A) \cup (B \cap A^c) \eqasb A\cup (B \cap A^c) \Rightarrow P(B) \eqthreei P(A) + P(B\cap A^c) = P(A) + P(B \backslash A) \Rightarrow (1.3)$.
        Beachte: $P(B \backslash A) = P(B)-P(A)$ gilt im Allgemeinen nicht ohne die Voraussetzung $A \subset B$.
        \newline \textit{Bsp.:} $A=\Omega$, $B=\emptyset \Rightarrow P(B \backslash A) = P(\emptyset \cap \Omega^c) = P(\emptyset) \eqonetwo 0$; $P(B)-P(A) = P(\emptyset)-P(\Omega) = 0-1 = -1 \neq 0$
    \item[1.4] Falls $A \subset B$, dann $ P(A) \leq P(B)$ \textit{(Monotonie)}, denn: Falls $A \subset B$, dann $P(B)-P(A) \eqonethree P(B \backslash A) \geqonei 0$
    \item[1.5] Verallgemeinerung von (1.3): $P(B \backslash A) = P(B) - P(A\cap B)$, 
        denn: $P(B) = P(B\cap\Omega) =P(B \cap (A \cup A^c)) = P((B\cap A) \cup (B\cap A^c)) = P(A\cap B) + P(B\cap A^c) = P(A\cap B) + P(B \backslash A) \Rightarrow (1.5)$ 
    \item[1.6] $P(A\cup B) = P(A)+P(B) - P(A\cap B)$, denn: $(A\cup B) \backslash B = (A \cup B) \cap B^c = (A \cup B) \cap B^c = (A \cap B^c) \cup (B \cap B^c) = A \backslash B $ und 
        $A (A\cap B) = A \cap (A \cap B)^c = A \cap (A^c \cup B^c) = (A \cap A^c) \cup ( A \cap B^c) = A \backslash B$. 
        Daher ist $(A \cup B) \backslash B = A \backslash (A\cap B) \Rightarrow P((A\cup B) \backslash B) = P(A \backslash (A \cap B)) 
        \Raonethree P(A \cup B) - P(B) = P(A) - P(A\cap B) \Rightarrow (1.6)$
    \item[1.7] Erweiterung von 1.6: $P(A\cup B\cup C) = P(A) + P(B) + P(C) - P(A\cap B) -P(A\cap C) -P(B\cap C) + P(A\cap B\cap C)$
    \item[1.8] Für alle Ereignisse $A_1,A_2,\dots$ gilt
        \newline $P(\bigcup_{i=1}^{n}A_i) \leq \sum_{i=1}^{n}P(A_i)$
        \newline $P(\bigcup_{i=1}^{\infty}A_i) \leq \sum_{i=1}^{\infty}P(A_i)$
        \newline Dies ist als \textbf{Boolesche Ungleichung} bekannt. Bei Disjunktheit gilt Gleichheit. Beweis in der Übung.
\end{itemize}

\vspace{6pt}
\noindent\textbf{Bsp. 1.9:} Ein Geschäft verkauft zwei Produkte. Für die zwei Ereignisse $A_i$: \glqq Produkt ist (am Abend) ausverkauft\grqq, $i=1,2$ ist bekannt: $P(A_1) = \frac{1}{8}$, $P(A_2) = \frac{1}{10}$, $P(A_1 \cap A_2) = \frac{1}{20}$.
Wie groß sind die W'keiten von:
\begin{itemize}
    \item[$B_1$]: \glqq Produkt 1 ausverkauft, aber Produkt 2 nicht\grqq?    \textit{(Lös: $\frac{3}{40}$)}
    \item[$B_2$]: \glqq genau ein Produkt ausverkauft\grqq?    \textit{(Lös: $\frac{5}{40}$)}
    \item[$B_3$]: \glqq mindestens ein Produkt ausverkauft\grqq?    \textit{(Lös: $\frac{7}{40}$)}
    \item[$B_4$]: \glqq höchstens ein Produkt ausverkauft\grqq?    \textit{(Lös: $\frac{19}{20}$)}
\end{itemize}

\vspace{6pt}
\noindent\textbf{Bem. 1.1:} Ist $\Omega$ nicht diskret, so ist es aus mathematischen Gründen in manchen Andwendungen nicht möglich, das W.-Maß P für alle Teilmengen von $\Omega$ zu definieren. 
\newline Jedoch lässt sich der Definitionsbereich von P stets so groß wählen, dass alle praktisch interessanten Teilmengen von $\Omega$ erfasst sind. 
Ist $\Omega$ diskret, so lässt sich das W.-Maß stets für alle Teilmengen von $\Omega$ definieren (vgl. Satz 1.1).

\section{Bedingte Wahrscheinlichkeiten und Unabhängigkeit}

\subsection{Bedingte Wahrscheinlichkeiten}

Oft von Interesse: W'keit des Ereignisses unter der Annahme, dass ein anderes Ereignis eintritt bzw. eingetreten ist. 
Dafür definieren und berechnen wir sog. \textit{bedingte Wahrscheinlichkeiten.}

\vspace{6pt}
\noindent\textbf{Bsp. 2.1:} 100 Personen
\newline\noindent
\begin{tabular}{c|c|c|c}
& mit Bachelorabschluss & ohne Bachelorabschluss & gesamt:\\
\hline
hohes Einkommen & 21 & 19 & 40\\
\hline
niedriges Einkommen & 15 & 45 & 60\\
\hline
& 36 & 64 & 100\\
\end{tabular}

\noindent Wird von den 100 Personen eine zufällig ausgewählt, ergeben sich für die Ereignisse 
H: \glqq hohes Einkommen\grqq\ und B: \glqq mit Bachelorabschluss\grqq\ die Laplace-W'keiten:
\newline $P(H)=\frac{40}{100}$, $P(B)=\frac{36}{100}$, $P(H\cap B)=\frac{21}{100}$

\vspace{4pt}
\noindent Intuitiv: W'keit für H, wenn bekannt ist, dass die Person einen Bachelorabschluss hat.
\newline Quotient: Anzahl der Personen mit hohem Einkommen von denen mit Bachelorabschluss / Anzahl der Personen mit Bachelorabschluss $=\frac{21}{36}$. Bedingte W'keit ist also hier:
\[\frac{|A\cap B|}{|B|}=\frac{|A\cap B|\backslash\Omega}{|B|\backslash\Omega}=\frac{P(A\cap B)}{P(B)}\]

\vspace{6pt}
\noindent Entsprechend definiert man allgemeiner

\vspace{6pt}
\noindent\textbf{Def. 2.1:} Sei $\Omega$ eine Ergebnismenge und P ein W'Maß auf $\Omega$. $B\subset\Omega$ sei ein EReignis mit $P(B)>0$. Dann heißt 
\[P(A|B):=\frac{P(A\cap B)}{P(B)}, A\subset\Omega\]
die \textit{bedingte Wahrscheinlichkeit} von A unter der Bedingung B.

\vspace{6pt}
\noindent\textit{In Bsp. 2.1}:
\begin{align*}
P(H|B) &=\frac{P(H\cap B)}{P(B)}=\frac{21}{36}\\
P(B|H) &=\frac{P(B\cap H)}{P(H)}=\frac{21}{40}
\end{align*}

\vspace{6pt}
\noindent\textbf{Bem. 2.1:} Manchmal ist es einfach, direkt $P(A|B)$ und $P(B)$ zu bestimmen, um damit $P(A\cap B)$ zu bestimmen: $(A\cap B)=P(B) \cdot P(A|B)$

\vspace{6pt}
\noindent\textbf{Bsp. 2.2:} Eine Urne enthält s schwarze und r rote Kugeln. 
Zwei Kugeln werden nacheinander ohne Zurücklegen entnommen. Es seien
\newline $A_1$: \glqq erste entnommene Kugel ist schwarz\grqq
\newline $A_2$: \glqq zweite entnommene Kugel ist rot\grqq
\newline Berechne $P(A_1\cap A_2)$:
\begin{align*}
P(A_1)&=\frac{s}{r+s}\\
P(A_2|A_1)&=\frac{r}{r+s-1}\\
P(A_2\cap A_1)&= P(A_1)\cdot P(A_2|A_1)=\frac{s\cdot r}{(r+s)(r+s-1)}
\end{align*}

\noindent Verallgemeinerung von 2.1:

\subsubsection{Multiplikationssatz für bedingte Wahrscheinlichkeiten}
\noindent Für n Ereignisse $A_1,\dots,A_n\subset\Omega$ gilt 
\[P(A_1\cap A_2\cap\dots\cap A_n)=P(A_1)\cdot P(A_2|A_1)\cdot P(A_3|A_1\cap A_2)\cdot\dots\cdot P(A_n|A_1\cap\dots\cap A_{n-1})\] 
sofern $P(A_1\cap\dots\cap A_{n-1})>0$.

\vspace{6pt}
\noindent Denn: Die Bedingung  $P(A_1\cap\dots\cap A_{n-1})>0$ stellt sicher, dass die bed. W'keiten definiert sind, 
und es gilt 
\begin{align*}
& P(A_1)\cdot P(A_2|A_1)\cdot P(A_3|A_1\cap A_2)\cdot\dots\cdot P(A_n|A_1\cap\dots\cap A_{n-1})\\
=& P(A_1)\cdot \frac{P(A_1\cap A_2)}{P(A_1)}\cdot \frac{P(A_1\cap A_2\cap A_3)}{P(A_1\cap A_2)}\cdot\dots\cdot \frac{P(A_1\cap\dots\cap A_n)}{P(A_1\cap\dots\cap A_{n-1})}\\
=& P(A_1\cap\dots\cap A_n)\\
\end{align*}

\vspace{6pt}
\noindent\textbf{Bem. 2.2:} Betrachtet man bei festem $B\subset\Omega$ mit $P(B)>0$ $P(A|B)$ als Funktion von A, dann ist $P(A|B)$ ein Wahrscheinlichkeitsmaß auf $\Omega$ (s.Tut., A14).
\newline Daher lassen sich auch für $P(A|B)$ die Eigenschaften (1.1)-(1.8) benutzen. Beispielsweise gilt
\begin{align*}
P(A^c|B) &= 1-P(A|B)\\
P(A_1|B) &\leq P(A_2|B) \mathrm{, falls } A_1\subset A_2\\
P(\emptyset|B) &= 0\\
p(A_1\cup A_2|B)&=P(A_1|B)+P(A_2|B)-P(A_1\cap A_2|B)
\end{align*}

\subsubsection{Satz von der totalen Wahrscheinlichkeit}
\noindent Seien $A_1,\dots,A_n$ Ereignisse, die eine Zerlegung von $\Omega$ bilden [d.h. $A_1\cup A_2\cup\dots\cup A_n = \Omega$ und $A_i\cap A_j = \emptyset$ für $i\neq j$]
und es sei $P(A_i)>0$ für $i=1,\dots,n$. Dann gilt
\[P(B)=\sum_{i=1}^{n}P(B|A_i)\cdot P(A_i)\text{ für alle Ereignisse }B\subset\Omega\]
Denn: 
\begin{align*}
\sum_{i=1}^{n}P(B|A_i)\cdot P(A_i) &= \sum_{i=1}^{n}P(B\cap A_i)\\
&= P(\bigcup_{i=1}^{n}(B\cap A_i))\\
&= P(B\cap(\bigcup_{i=1}^{n}A_i))\\
&= P(B)
\end{align*}

\subsubsection{Satz von Bayes}
\noindent Seien $A_1,\dots,A_n$ Ereignisse, die eine Zerlegung von $\Omega$ bilden und es sei $P(A_i)>0$ für $i=1,\dots,n$. Dann gilt für jedes $B\subset\Omega$ mit $P(B)>0$
\[P(A_k|B)=\frac{P(B|A_k)\cdot P(A_k)}{\sum_{i=1}^{n}P(B|A_i)\cdot A_i} \hspace{20pt} \text{für } k=1,\dots,n\]
Denn:
\begin{align*}
\frac{P(B|A_k)\cdot P(A_k)}{\sum_{i=1}^{n}P(B|A_i)\cdot A_i} & \hspace{4pt}\stw\hspace{4pt} \frac{P(B|A_k)\cdot P(A_k)}{P(B)}\\
& \hspace{4pt}\Btuwon\hspace{4pt} \frac{P(B\cap A_k)}{P(B)}\\
& \hspace{4pt}\Dtuwon\hspace{4pt} P(A_k|B)\\
\end{align*}

\vspace{6pt}
\noindent\textbf{Bsp. 2.3:} Betrachte für eingehende Emails die folgenden 3 Ereignisse:
\newline $A_1$: \glqq Spam\grqq
\newline $A_2$: \glqq niedrige Priorität\grqq
\newline $A_3$: \glqq hohe Priorität\grqq
\newline $A_1,A_2,A_3$ bildet eine Zerlegung. Es gil $P(A_1)=0,6$, $P(A_2)=0,3$, $P(A_3)=0,1$. 
\newline Für das Ereignis B: \glqq E-mail enthält das Wort ,gratis'\grqq gelte $?(B|A_1)=0,8$, $P(B|A_2)=0,05$, $P(B|A_3)=0,05$. (W'keiten wurden durch Beobachtung des Postfachs errechnet.)
\newline $\Rightarrow$ Was ist P(B)?

\vspace{4pt}
\noindent Von dem Satz der totalen W'keit gilt
\begin{align*}
P(B) &= P(B|A_1)\cdot P(A_1) + P(B|A_2)\cdot P(A_2) + P(B|A_3)\cdot P(A_3)\\
&= 0,8\cdot0,6+0,05\cdot0,3+0,05\cdot0,1\\
&= 0,5
\end{align*}

Angenommen, eine E-mail ist eingegangen, die das Wort \glqq gratis\grqq\ enthält. Wie groß ist die W'keit, dass es sich um Spam handelt?
\[P(A_1|B)=\frac{P(B|A_1)\cdot P(A_1)}{\sum_{i=1}^{3}P(B|A_i)=\frac{0,8\cdot0,6}{0,5}=0,96}\]
Analog,
\begin{align*}
    P(A_2|B)&=\frac{P(B|A_2)\cdot P(A_2)}{\sum_{i=1}^{3}P(B|A_i)}=\frac{0,05\cdot0,3}{0,5}=0,03\\
    P(A_3|B)&=\frac{P(B|A_3)\cdot P(A_3)}{\sum_{i=1}^{3}P(B|A_i)}=\frac{0,05\cdot0,1}{0,5}=0,01\\
\end{align*}

\vspace{6pt}
\noindent\textbf{Bem. 2.2:} In den Situationen des Satzes von Bayes ($A_1,\dots,A_n$ bilden eine Zerlegung von $\Omega$, $B\subset\Omega$, $P(B)>0$)
bezeichnet man $P(A_1),P(A_2),\dots,P(A_n)$ auch als \textit{a-priori-Wahrscheinlichkeiten} und $P(A_1|B),P(A_2|B),\dots,P(A_n|B)$ als \textit{a-posteriori-Wahrscheinlichkeiten}.

\vspace{6pt}
\noindent\textbf{Bsp. 2.4:} Ein medizinischer Test für eine Krankheit liefert bei erkrankten Personen mit hoher W'keit 0,99 das richtige Resultat \glqq positiv\grqq. Bei Gesunden liefert der Test mit geringer W'keit 0,02 das falsche Resultat \glqq positiv\grqq. 
0,3\% der Personen aus der Bevölkerung sind krank (a priori). Wie groß ist die bedingte W'keit, dass jemand krank ist (a posteriori), wenn das Testergebnis positiv ist?

\noindent Seien
\newline $A_1$: \glqq krank\grqq
\newline $A_2$: \glqq gesund Priorität\grqq
\newline $B$: \glqq positiv\grqq
\newline Geg. also $P(A_1)=0,003$ (a priori), $P(A_2)=1-0,003=0,997$, $P(B|A_1)=0,99$, $P(B|A_2)=0,02$.
\newline Formel von Bayes liefert
\begin{align*}
P(A_1|B) &= \frac{P(B|A_1)\cdot P(A_1)}{P(B|A_1)P(A_1)+P(B|A_2)P(A_2)}\\
&= \frac{0,99\cdot0,03}{0,99\cdot0,003+0,02\cdot0,997}\\
&= 0,13 \hspace{20pt} \text{(a posteriori)}
\end{align*}

\subsection{Unabhängigkeit}
\noindent Intuitiv: Ereignisse A und B sind unabhängig, falls die Kenntnis über das Eintreten des einen keine Information über die W'keit des Eintretens des anderen liefert:
\newline $P(A|B)=P(A)$ und $P(B|A)=P(B)$, $P(A),P(B)>0$. Beide Gleichungen sind Äquivalent zu $P(A\cap B)=P(A)\cdot P(B)$. Nehme diese Gleichung für die Definition:

\vspace{6pt}
\noindent\textbf{Def. 2.2:} Zwei Ereignisse $A,B\subset\Omega$ heißen \textit{unabhängig}, wenn
\[P(A\cap B)=P(A)\cdot P(B)\]

\vspace{6pt}
\noindent\textbf{Bsp. 2.5:} Zweimaliger Würfelwurf. Der erste Wurf ist unabhängig vom Zweiten. 
\newline Also sind z.B.
\newline $A_1$: \glqq im ersten Wurf 6\grqq
\newline $A_2$: \glqq im zweiten Wurf mind. 3\grqq
\newline unabhängige Ereignisse und $P(A_1\cap A_2)=P(A_1)\cdot P(A_2)=\frac{1}{6}\cdot\frac{2}{3}=\frac{1}{9}$ anwendbar.

\vspace{6pt}
\noindent\textbf{Bem. 2.4:} Sind A und B unabhängig, dann gilt auch
\begin{itemize}
    \item[(i)] $A$ und $B^c$ sind unabhängig.
    \item[(ii)] $A^c$ und $B$ sind unabhängig.
    \item[(iii)] $A^c$ und $B^c$ sind unabhängig.
\end{itemize}

\noindent Nachweis von (i):
\begin{align*}
P(A\cap B^c) &\eqonefive P(A)-P(A\cap B)\\
&= P(A) - P(A)\cdot P(B)\\
&= P(A)\cdot(1-P(B))\\
&= P(A)\cdot P(B^c)
\end{align*}

\vspace{6pt}
\noindent\textbf{Def. 2.3:} Sei $A_1,A_2,\dots$ eine endliche oder unendliche Folge von Ereignissen.
$A_1,A_2,\dots$ heißen \textit{paarweise unabhängig}, falls
\[P(A_i\cap A_j)=P(A_i)\cdot P(A_j)\]
für jedes Paar von Indizes $i\neq j$.
\newline $A_1,A_2,\dots$ heißen \textit{unabhängig}, falls für jede endliche Auswahl von verschiedenen Indices $i_1,\dots,i_k$ gilt 
\[P(A_{i_1}\cap\dots\cap A_{i_k})=P(A_{i_1})\cdot\dots\cdot P(A_{i_k})\]
\newline \textbf{Bem.2.5:} Unabhängigkeit $\Rightarrow$ paarweise Unabhängigkeit, aber die Implikation gilt NICHT in die andere Richtung. 
Paarweise Unabhängigkeit ist außerdem oft zu schwach, um interessante Resulatate zu erhalten.

\vspace{6pt}
\noindent\textbf{Bsp.2.6:} Zweimaliger Würfelwurf.
\newline A: \glqq erste Augenzahl gerade\grqq\ 
\newline B: \glqq zweite Augenzahl ungerade\grqq\
\newline C: \glqq beide Augenzahlen gerade oder beide ungerade\grqq\
\newline $P(A)=\frac{1}{2}$, $P(B)=\frac{1}{2}$, $P(C)=\frac{1}{2}$: 
\begin{align*}
P(A\cap B) &= \frac{3*3}{36}=\frac{1}{4}=P(A)\cdot P(B)\\
P(A\cap C) &= \frac{3*3}{36}=\frac{1}{4}=P(A)\cdot P(C)\\
P(B\cap C) &= \frac{3*3}{36}=\frac{1}{4}=P(B)\cdot P(C)\\
\end{align*}
\newline $\Rightarrow$ $A,B,C$ sind paarweise unabhängig. Aber $A\cap B\cap C=\emptyset \Rightarrow P(A\cap B\cap C)=0<P(A)\cdot P(B)\cdot P(C) \Rightarrow A,B,C$ sind nicht unabhängig.

\vspace{6pt}
\noindent\textbf{Bem.2.6:} Sind $A_1,\dots,A_n$ unabhängige Ereignisse, und ist für jedes $i\in{1,\dots,n} B_i=A_i$ oder $B_i=A_i^c$, dann sind $B_1,\dots,B_n$ unabhängig.

\vspace{6pt}
\noindent\textbf{Bsp.2.7:} Drei Personen werden gefragt, ob sie einem bestimmten Vorschlag zustimmen. Jede Person antwortet mit W'keit 0,8 \glqq nein\grqq\ und mit W'keit 0,2 \glqq ja\grqq\.
Die Antworten sind unabhängig. Wie groß ist dann die W'keit, dass alle drei Personen dieselbe Antwort geben?

\vspace{4pt}
\noindent Sei $A_i$ das Ereignis. Person i antwortet \glqq ja\grqq\, $i=1,2,3$. Dann gilt $P(A_1)=P(A_2)=P(A_3)=0,2$ und $A_1,A_2,A_3$ sind unabhängig. 
\newline Gesuchte W'keit ist 
\begin{align*}
P([A_1\cap A_2 \cap A_3] \cup [A_1^c \cap A_2^c \cap A_3^c]) &= P(A_1\cap A_2 \cap A_3)+P(A_1^c \cap A_2^c \cap A_3^c)\\ 
&= P(A_1)P(A_2)P(A_3)+P(A_1^c)P(A_2^c)P(A_3^c)\\
&= 0,2^3+0,8^3=0,52\\
\end{align*}

\section{Zufallsvariablen}

\subsection{Zufallsvariablen}
\noindent Eine \textit{Zufallsvariable} beschreibt eine reellwertige Größe, die vom Zufall, d.h. von $\omega$ abhängt. Genauer:

\vspace{6pt}
\noindent\textbf{Def. 3.1}: Sei $\Omega$ eine Ergebnismenge. Eine \underline{Zufallsvariable} (ZV) X ist eine Abbildung von $\Omega$ nach $\mathbb{R}$, $X:\Omega \rightarrow \mathbb{R}$.

\vspace{6pt}
\noindent\textbf{Bsp. 3.1:}\label{bsp3.1} Zweimaliger Münzwurf
\begin{center}
\begin{tabular}{c|c}
    $\Omega$ & $X(w)=$Anzahl von K\\
    \hline
    $\omega_1=(K,K)$ & $X(\omega_1)=2$\\
    $\omega_2=(K,K)$ & $X(\omega_2)=1$\\
    $\omega_3=(K,K)$ & $X(\omega_3)=1$\\
    $\omega_4=(K,K)$ & $X(\omega_4)=0$\\
\end{tabular}
\end{center}
Ist $P(\{\omega\})=\frac{1}{|\Omega|}=\frac{1}{4}$ für alle $\omega\in\Omega=\{\omega_1,\dots,\omega_4\}$, dann lassen sich die W'keiten 
\begin{align*}
    P(X=n) &= \text{W'keit, dass die ZV X den Wert n annimmt}\\
    &= \text{W'keit, dass n-mal Kopf fällt}\\
    &= P(\{\omega\in\Omega:X(\omega)=n\}) n=0,1,2,\dots
\end{align*}
berechnen.
\begin{align*}
P(X=0) &= P(\{\omega_4\})=\frac{1}{4}\\
P(X=1) &= P(\{\omega_2,\omega_3\})=\frac{1}{2}\\
P(X=2) &= P(\{\omega_1\})=\frac{1}{4}\\
P(X=3) &= P(\emptyset)=0
\end{align*}

\vspace{6pt}
\noindent Notation: Für ZV X,Y
\begin{align*}
\{X=a\} &= \{\omega\in\omega: X(\omega)=a\}, \hspace{10pt}a\in\mathbb{R}\\
\{X < a\} &= \{\omega\in\Omega: X(\omega)<a\} \\
\{X \in A\} &= \{\omega\in\Omega: X(\omega)\in A\}, \hspace{10pt}A\in\mathbb{R}\\
\{X\in A, Y\in B\} &= \{\omega\in\Omega: X(\omega)\in A \text{ und } Y(\omega)\in B\}, \hspace{10pt}A,B\in\mathbb{R}\\
P(X\geq a) &= P(\{\omega\in\Omega: X(\omega)\geq a\})\\
P(X\leq a, Y\leq b) &= P(\{\omega\in\Omega: X(\omega)\leq a \text{ und } Y(\omega)\leq b\}), \hspace{10pt}a,b\in\mathbb{R}\\
P(X>Y) &= P(\{\omega\in\Omega: X(\omega)>Y(\omega)\})\\
\end{align*}
usw. 

\subsection{Unabhängigkeit}
\noindent Bisher: Unabhängigkeit von Ereignissen.
\newline Jetzt: Unabhängigkeit von Zufallsvariablen

\vspace{6pt}
\noindent\textbf{Def. 3.2:} 
\newline (a) ZV $X_1,\dots,X_n$ heißen unabhängig, falls
\[ (*) \hspace{20pt} P(X_1\in B_1,\dots,X_n\in B_n)=P(X_1\in B_1)\cdot\dots\cdot P(X_n\in B_n)\] für alle $B_1,\dots,B_n \in \mathbb{R}$.
\newline (b) Ist $X_1,X_2,\dots$ eine unendliche Folge von ZV, dann heißen $X_1,X_2,\dots$ unabhängig, falls $(*)$ für alle $n\geq 2$ gilt.

\vspace{6pt}
\noindent\textbf{Bem. 3.1:} Sind $X_1,\dots,X_n$ unabhängige ZV und sind $A_1,\dots,A_n$ Ereignisse, so dass Eintreten von $A_i$ nur von $X_i$ abhängt, $i=1,\dots,n$, dann sind $A_1,\dots,A_n$ unabhängig.

\vspace{6pt}
\noindent\textbf{Bsp. 3.2:} Seien $X_1,X_2,X_3$ unabhängige ZV. Dann sind die Ereignisse $A_1=\{X_1<3\}$, $A_2=\{X_2^2>7\}$, $A_3=\{4<|X_3|<7\}$ unabhängig.
\newline Wählt man in (*) z.B. $B_1=\{x\in \mathbb{R}:x<3\}$, $B_2=\{x\in \mathbb{R}: x^2<7\}$, $B_3 = \mathbb{R}$ ergibt sich:
\begin{align*}
    P(A_1\cap A_2) &= P(X_1\in B_1,X_2\in B_2,X_3\in B_3) \\
    &= P(X_1\in B_1) \cdot P(X_2\in B_2) \cdot P(X_3\in B_3)\\ 
    &= P(A_1)\cdot P(A_2)\cdot1\\
\end{align*}

\subsubsection{Satz 3.1:} $X_1,\dots,X_n$ seien ZV, die jeweils höchstens abzählbar viele Werte annehmen. Dann gilt: $X_1,\dots,X_n$ sind unabhängig, gdw 
\[P(X_1=b_1,\dots,X_n=b_n) = P(X_1=b_1)\dots P(X_n=b_n)\] 
für alle $b_1,\dots,b_n\in\mathbb{R}$.

\vspace{6pt}
\noindent\textbf{Bsp. 3.3:} 
$X_1$ sei ZV mit $P(X_1=-1)=P(X_1=0)=P(X_1=1)=\frac{1}{3}$ und ZV $X_2$ sei definiert durch $X_2=X_1^2+2$.
\newline Für $b_1=0$ und $b_2=2$ gilt 
\begin{align*}0 
P(X_1=b_1)&=P(X_1=0)=\frac{1}{3}\\
P(X_2=b_2)&=P(X_1^2+2=2)=P(X_1=0)=\frac{1}{3}\\
%\Rightarrow P(X_1=b_1,X_2=b_2)&=P(X_1=0,X_1^2+2=2)\\
%&=P(X_1=0)=\frac{1}{3}
\end{align*}
\begin{align*}
\Rightarrow P(X_1=b_1,X_2=b_2)&=P(X_1=0,X_1^2+2=2)\\
&=P(X_1=0)=\frac{1}{3}
\end{align*}
und daher $P(X_1=b_1,X_2=b_2)\neq P(X_1=b_1)\cdot P(X_2=b_2)$.
\newline $\Rightarrow X_1$ und $X_2$ sind nicht unabhängig.

\vspace{6pt}
\noindent\textbf{Bsp. 3.4:} n-maliger Würfelwurf (mit Laplace-Würfel)
\newline $\Omega=\{\omega=(\omega_1,\dots,\omega_n): \omega_i\in\{1,\dots,6\}; 1\leq i\leq n\}$
\newline $|\Omega| = 6^n, P(A)= \frac{|A|}{|\Omega|}$ für alle $A\in\Omega$
\newline $X_i=$ Ergebnis des i-ten Wurfs
\newline $X_i((\omega_1,\dots,\omega_n))=\omega_i$
\newline Behauptung: $X_1,\dots,X_n$ sind unabhängig. Nach Satz 3.1 reicht es zu zeigen:
\newline $P(X_1=b_1,\dots,X_n=b_n)=P(X_1=b_1)\cdot\dots\cdot P(X_n=b_n) \hspace{3pt}\forall \hspace{2pt}b_1,b_2,\dots,b_n\in\mathbb{R}$.
\vspace{4pt}
\newline\textbf{Fall 1:} Ein $b_i$ ist keine mögliche Augenzahl $\Rightarrow$ beide Seiten = 0
\newline\textbf{Fall 2:} $b_1,\dots,b_n\in\{1,\dots,6\}$:
\begin{align*}
P(X_1=b_1,\dots,X_n=b_n) &= P(\{\omega\in\Omega: X_1(\omega)=b_1,\dots,X_n(\omega)=b_n\})\\
&= P(\{\omega\in\Omega: \omega_1=b_1,\dots,\omega_n=b_n\})\\
&= P(\{b_1,\dots,b_n\})\\
&= \frac{1}{|\Omega|} = \frac{1}{6^n}
\end{align*}
und 
\begin{align*}
P(X_i=b_i) &= P(\{\omega\in\Omega:X_i(\omega)=b_i\})\\
&= P(\{\omega\in\Omega:\omega_i=b_i\})\\
&= \frac{6^{n-1}}{6^n}=\frac{1}{6}, i=1,\dots,6
\end{align*}
$\Rightarrow P(X_1=b_1)\cdot\dots\cdot P(X_n=b_n)=\frac{1}{6^n}=P(X_1=b_1,\dots,X_n=b_n)$. 
\newline Es folgt, dass $X_1,\dots,X_n$ unabhängig sind.

\subsection{Verteilungsfunktionen, W'keitsfunktionen und Dichten}

\noindent\textbf{Def. 3.3:} Sei X eine ZV. 
Die Verteilungsfunktion (VF) $F=F_x$ von x ist definiert durch $F(x)=F_x(x)=P(X\leq x) \hspace{3pt}\forall\hspace{2pt} x\in\mathbb{R}$.

\vspace{6pt}
\noindent\textbf{Bsp. 3.5:} Für X wie in Bsp. \ref{bsp3.1}: $P(X=0)=\frac{1}{4}, P(X=1)=\frac{1}{2}, P(X=2)=\frac{1}{4}$.

\begin{equation*}
\Rightarrow F_x(x)=P(X\leq x)=\begin{cases}
P(\emptyset)=0 & x<0\\
P(X=0)=\frac{1}{4} & 0\leq x<1\\
P(X=0)+P(X=1)=\frac{3}{4} & 1\leq x<2\\
1 & x\geq2
\end{cases}
\end{equation*}
        
\begin{tikzpicture}
    \draw[step=1cm,gray,very thin] (0,0) grid (6,6);
    \draw[thick,->] (0,0) -- (6,0) node[anchor=north west] {x};
    \draw[thick,->] (0,0) -- (0,6) node[anchor=south east] {$F_x(x)$};
    \foreach \x in {1,2}
        \draw(2*\x,1pt) -- (2*\x,-1pt) node[anchor=north] {$\x$};
    \foreach \y in {0.25,0.5,0.75,1}
        \draw(1pt,6*\y) -- (-1pt,6*\y) node[anchor=east] {$\y$};
    \draw[red,thick] (0,1.5) -- (2,1.5);
    \draw[red,thick] (2,4.5) -- (4,4.5);
    \draw[red,thick] (4,6) -- (6,6);
    \fill[red] (0,1.5) circle (0.1cm);
    \fill[red] (2,4.5) circle (0.1cm);
    \fill[red] (4,6) circle (0.1cm);
\end{tikzpicture}

\subsubsection{Satz 3.2: Eigenschaften von VF} 
Sei X eine ZV und F ihre VF. Dann gilt
\begin{itemize}
    \item[(a)] F ist monoton wachsend.
    \newline Für alle $x,y\in\mathbb{R}$ mit $x<y$ gilt $F(x)\leq F(y)$.
    \item[(b)] $\lim_{x\rightarrow-\infty}F(x)=0, \lim_{x\rightarrow\infty}F(x)=1$
    \item[(c)] F ist rechtsseitig stetig.
    \item[(d)] $P(X>x)=1-F(x), x\in\mathbb{R}$
    \item[(e)] $P((a<X\leq b))= $
    \item[(f)] $P(X<x)=F(x-)$, wobei $F(x-)=lim_{y\rightarrow x}F(y)$ der linksseitige Grenzwert von F an der stelle x ist, d.h., der Grenzwert der Werte $F(y)$ wenn y sich x von links nähert.
    \item[(g)] $P(X=x)=F(x)-F(x-) =$ Sprunghöhe von F an der Stelle X
    \newline Insb.
    \newline
    \begin{tikzpicture}
        \draw[step=1cm,gray,very thin] (0,0) grid (4,4);
        \draw[thick,->] (0,0) -- (4,0) node[anchor=north west] {x};
        \draw[thick,->] (0,0) -- (0,4) node[anchor=south east] {$F(x)$};
        \draw(2,1pt) -- (2,-1pt) node[anchor=north] {$x_c$};
        \draw(1pt,1.5) -- (-1pt,1.5) node[anchor=east] {$P(X<x_c)$};
        \draw(1pt,2.5) -- (-1pt,2.5) node[anchor=east] {$P(X\leq x_c)$};
        \fill[red] (2,2.5) circle (0.1cm);
        \draw[red] (2,1.5) arc (-15:-85:2);
        \draw[red] (2,2.5) arc (180:110:1.3);
    \end{tikzpicture}
    \newline $P(X=x)=P(X\leq x_c)-P(X<x_c)$ (s. Sprung in der Grafik)
    \newline $P(X=x)=0 \Leftrightarrow$ F ist stetig an der Stelle X
    \newline $P(X=x)=0$ für alle $x\in\mathbb{R} \Leftrightarrow$ F ist stetig auf $\mathbb{R}$
    \item[(h)] $P(X\geq x)=1-F(x-)$
    \newline $ P(X\leq X \leq b) = F(b)-F(a-), \hspace{15pt} a\leq b$
\end{itemize}

\vspace{6pt}
\noindent\textbf{Beweis:} 
\begin{itemize}
\item[(a),(e)] Für $a<b$ gilt $0\leq P(a<X\leq b) = P(\{X\leq b\}\backslash \{X\leq a\}) = $
    \newline $= P(X\leq b) - P(X\leq a) = F(b) - F(a)$
\item[(b),(c)] Übungsaufgabe 21
\item[(d)] $P(X>x) = P(\{X\leq x\}^c) = 1-P(X\leq x)=1-F(x)$
\item[(f)] $P(X<x)=P(\bigcup_{n=1}^{\infty}\{X\leq x-\frac{1}{n}\})$
    \newline Übungsaufgabe 20 $= \lim_{n\rightarrow\infty}P(X\leq x-\frac{1}{n}) = \lim_{n\rightarrow\infty}F(x-\frac{1}{n}) = F(x-)$
    \newline $\{X\leq x-\frac{1}{n}\}\subset\{X\leq x-\frac{1}{n+1}\}$
\item[(h)] $P(X\leq x) = P(\{X<x\}^c)=1-P(X<x) \fover 1-F(x-)$
    \newline $P(a\leq X\leq b) = P(\{x\leq b\} \backslash \{X<a\}) = P(X\leq b) - P(X<a) = F(b) - F(a-)$.
    \newline Für $a=b=x$ ergibt sich (g).
\end{itemize}

\vspace{6pt}
\noindent\textbf{Bem. 3.2:} Man kann zeigen: 
\begin{itemize}
\item[(a)] Jede Funktion $F:\mathbb{R}\rightarrow\mathbb{R}$, die $(a)-(c)$ aus Satz 3.2 erfüllt, ist die VF einer geeigneten ZV.
\item[(b)] Seien $X$ und $Y$ ZF mit VF $F_x$ und $F_y$. Dann gilt
    \newline $F_x(x)=F_y(x)$ für alle $x\in\mathbb{R} \Leftrightarrow P(X\in B) = P(Y\in B)$ für alle $B\in\mathbb{R}$.
    \newline Falls $F_x(x)=F_y(x)$, dann sagt man \glqq X und Y sind identisch verteilt\grqq.
\end{itemize}

\subsubsection{Diskrete ZV}
\vspace{6pt}
\noindent\textbf{Def. 3.4:} Eine ZV X heißt \textit{diskret}, falls X höchstens abzählbar viele Werte $x_1,x_2,\dots$ annimmt. 
\newline Ist X eine diskrete ZV, dann heißt die durch 
\[f(x)=P(X=x) \hspace{15pt} x\in\mathbb{R},\]
definierte Funktion $f:\mathbb{R}\rightarrow\mathbb{R}$ die \textit{Wahrscheinlichkeitsfunktion} von X.

\vspace{6pt}
\noindent Sei X eine diskrete ZV mit W'funktion f und VF F. Bezeichne die möglichen Werte von X mit $x_1,x_2,\dots$, d.h., 
\[\{x_1,x_2,\dots\}=\{X(a):\omega\in\Omega\}\]
Dann gilt für jedes $B\in\mathbb{R}$,
\[P(X\in B)=P(\bigcup_{x\in B}\{X=x\})=\sum_{x_i\in B}P(X=x)=\sum_{x_i\in B}f(x_i)\]

\begin{align*}
\sum_{x_i\in\mathbb{R}}f(x_i) &= P(X\in\mathbb{R})=1 \\ 
F(x) &= P(X\leq x) = \sum_{x_i\leq x}f(x), \hspace{15pt}x\in\mathbb{R}\\
f(x) &= P(X=x)\hspace{15pt}\eqthreetwog \hspace{15pt} F(x)-F(x-) \hspace{15pt} x\in\mathbb{R}\\
\end{align*}

\vspace{6pt}
\noindent Setze
\[P_x(B)=P(X\in B) \text{ für jedes } B\in\mathbb{R}.\]
Dann ist $P_x$ ein W-Maß auf $\mathbb{R}$, denn:
\begin{itemize}
\item[(i)] $P_x(B)\geq 0$ für alle $B\subset\mathbb{R}$
\item[(ii)] $P_x(\mathbb{R})=P(X\in\mathbb{R})=1$
\item[(iii)] $\sigma$-Additivität
    \newline Seien $B_1,B_2,\dots\subset\mathbb{R}$ paarweise disjunkt
    \newline $\Rightarrow P_x(B_1\cup B_2\cup\dots)=P(X\in B_1\cup B_2\cup\dots) = P(\{X\in B_1\}\cup\{X\in B_2\}\cup\dots)$
    \newline $ \eqsigmaadd \hspace{12pt}\sum_{i}P(X\in B_i)=\sum_{i}P_x(B_i)$.
\end{itemize}
Das W-Maß $P_x$ heißt die Verteilung von X.

\subsubsection{Stetige ZV}
\vspace{6pt}
\noindent\textbf{Def. 3.5:} Eine ZV X heißt \textit{stetig}, falls es eine Funktion $f:\mathbb{R}\rightarrow\mathbb{R}$ gibt, so dass 
\begin{align*}
f(x) &\geq 0 \hspace{15pt}\forall x\in\mathbb{R},\\
\int_{-\infty}^{\infty}f(x)dx &= 1 \\
\text{und}\hspace{20pt}P(X\leq x) &= \int_{-\infty}^{\infty}f(u)du \hspace{15pt}\forall x\in\mathbb{R}.
\end{align*}
f heißt \textit{Dichte} oder \textit{Wahrscheinlichkeitsdichte} von X.

\vspace{6pt}
\noindent Sei X eine stetige ZV mit Dichte f und VF F
\newline $\rightarrow$ F ist stetig $\hspace{12pt}\impthreetwog\hspace{12pt} P(X=x)=0$ für alle $x\in\mathbb{R}$.
\newline $\Rightarrow$ Für $a\leq b$ gilt
\begin{align*}
P(a\leq X\leq b) &= P(X=a) + P(a<X<b) + P(X=b) = P(a<X<b)\\
&= P(a\leq X<b) = P(a<X\leq b) = F(b)-F(a)\\
&= \int_{-\infty}^{a}f(x)dx - \int_{-\infty}^{b}f(x)dx = \int_{a}^{b}f(x)dx\\
\end{align*}

\noindent Ist die Dichte f an der Stelle $x_c$ stetig, so gilt 
\[F'(x_c)=f(x_c)\]
f lässt sich aus F gewinnen.

\subsection{Beispiele diskreter Verteilungen}
\noindent\textit{Diskrete Verteilungen}, d.h. Verteilungen von diskreten ZV
\newline Einfachster Fall: $\exists a\in\mathbb{R}$ mit $P(X=a)=1$.

\begin{tikzpicture}
        \draw[step=1cm,gray,very thin] (0,0) grid (4,4);
        \draw[thick,->] (0,0) -- (4,0) node[anchor=north west] {x};
        \draw[thick,->] (0,0) -- (0,4) node[anchor=south east] {$F_x(x)$};
        \draw(2,1pt) -- (2,-1pt) node[anchor=north] {$a$};
        \draw(1pt,2) -- (-1pt,2) node[anchor=east] {$1$};
        \fill[red] (2,2) circle (0.1cm);
        \draw[red,thick] (0,0) -- (2,0);
        \draw[red,thick] (2,2) -- (4,2);
\end{tikzpicture}

\subsubsection{Bernoulli-Verteilung}
\noindent Eine ZV X, die nur die Werte 0 und 1 annimmt mit 
\[P(X=1)=p, P(X=0)=1-p\]
heißt \textit{Bernoulli-verteilt} mit Parameter p.
\newline X wird auch als Bernoulli-Variable bezeichnet. $(0\leq p \leq 1)$
\newline Bez. X$\sim$Ber(p).

\vspace{6pt}
\noindent Sei A ein Ereignis. Die Indikatorfunktion $I_A$ von A (oder Indikatorvariable) ist definiert durch
\begin{equation*}
    I_A(\omega)=\begin{cases}
        1, & \omega\in A\\
        0, & \omega\in A^c\\
    \end{cases}
\end{equation*}

\noindent Also
\newline $I_A(\omega)=1 \Leftrightarrow$ A tritt ein
\newline $X=I_A$ ist eine Bernoulli-Variable mit Parameter p=P(A).
\newline Man sagt oft, 
\begin{align*}
    \text{Erfolg tritt ein, falls } & X(\omega)=1\\
    \text{Misserfolg tritt ein, falls } & X(\omega)=0\\
\end{align*}

\begin{tikzpicture}
        \draw[step=1cm,gray,very thin] (0,0) grid (4,4);
        \draw[thick,->] (0,0) -- (4,0) node[anchor=north west] {x};
        \draw[thick,->] (0,0) -- (0,4) node[anchor=south east] {$F_x(x)$};
        \draw(2,1pt) -- (2,-1pt) node[anchor=north] {$1$};
        \draw(1pt,1) -- (-1pt,1) node[anchor=east] {$1-p$};
        \draw(1pt,2.5) -- (-1pt,2.5) node[anchor=east] {$1$};
        \fill[red] (2,2.5) circle (0.1cm);
        \draw[red] (0,1) -- (2,1);
        \draw[red] (2,2.5) -- (4,2.5);
\end{tikzpicture}

\subsubsection{Binomialverteilung}
\noindent Seien $X_1,\dots,X_n$ unabhängig und jeweils $~Ber(p)$ (Erfolgsw'keit jeweils p).
\newline Anzahl der Erfolge 
\[S_n:= X_1+\dots+X_n\]
Bsp.: n Münzwürfe
\begin{equation*}
    X_i=\begin{cases}
        1, & \text{im i-ten Wurf K}\\
        0, & \text{im i-ten Wurf Z}\\
    \end{cases}
    S_n = \text{Anzahl der K-Würfe}
\end{equation*}
$S_n$ = Anzahl der K-Würfe $\Rightarrow$ Mögliche Werte von $S_n:0,1,\dots,n.$ $P(S_n=k)=?$
\newline Da $X_1,\dots,X_n$ unabhängig sind, 
\begin{align*}
   & P(X_1=1,\dots,X_k=1, X_{k+1}=0,\dots,X_n=0)\\
   =& P(X_1=1)\cdot\dots\cdot P(X_k=1)\cdot P(X_{k+1}=0)\cdot\dots\cdot P(X_n=0)\\
   =& p^k \cdot q^{n-k} \hspace{15pt} (q:=1-p)
\end{align*}

\noindent $\rightarrow$ hängt nur ab von der Anzahl der vorgegebenen Einsen, nicht von deren Position. Also
\[P(S_n=k)=p^k \cdot q^{n-k}\]
\glqq Anzahl der Möglichkeiten, aus n Versuche k anzugeben, bei denen Erfolg eintreten soll\grqq\
\[\Rightarrow P(S_n=k)=\binom{n}{k}\cdot p^k\cdot q^{n-k}, k=0,1,\dots,n\]
wobei $\binom{n}{k}=\frac{n!}{k!(n-k!)}$ (Binomialkoeffizient).

\noindent Insb. $P(S_n=0)=\binom{n}{0}\cdot p^0\cdot q^n=q^n, P(S_n=1)=\binom{n}{n}\cdot p^n\cdot q^0=p^n$.
\newline $S_n$ heißt \textit{binomialverteilt} mit Parametern n(\# Versuchen) und p(Erfolgsw'keit).
\newline Bez. $S_n\sim$Bin(n,p)

\vspace{6pt}
\noindent\textbf{Bsp. 3.6:} Wie groß ist die W'keit, dass von 5 E-Mails höchstens eine Spam ist, wenn bei jeder E-Mail die W'keit für Spam $\frac{1}{3}$ ist?
\newline X=Anzahl der Spam E-Mails $\sim$Bin$(5,\frac{1}{3})$
\begin{align*}
P(X\leq 1) &= P(X=0)+P(X=1)\\
&= \binom{5}{0}\cdot (\frac{1}{3})^0\cdot (\frac{2}{3})^5 + \binom{5}{1}\cdot (\frac{1}{3})^1\cdot (\frac{2}{3})^5\\
&= \frac{2^5}{3^5}+\frac{5!}{1!4!}\cdot
\end{align*}

\subsubsection{Geometrische Verteilung}
Seien $X_1,X_2,\dots$ unabh., $\sim$Ber(p), $0<p\leq 1$
\newline $X_i=1 \leftrightarrow$ Erfolg im i-ten Versuch.
\newline T = Anuahl der Versuche bis zum ersten Erfolg (einschließlich)
\[P(T=1)=P(X_1=1)=p\]
\newline Für $k\geq 2$ ist
\begin{align*}
P(T=k) &= (X_1=0,\dots,X_{k-1}=0,X_k=1)\\
&= P(X_1=0)\cdot\dots\cdot P(X_{k-1}=0)\cdot P(X_k=1) = (1-p)^{k-1}\cdot p
\end{align*}
Mit $q=1-p$ also 
\[P(T=k)=q^{k-1}\cdot p, k=1,2,\dots\]
Bez. T$\sim$Geo(p) Geometrische Verteilung mit Parameter $0<p\leq1$.
\newline Beachte
\[\sum_{k=1}^{\infty}q^{k-1}\cdot p = p(1+q+q^2+\dots)=p\cdot \frac{1}{1-q}= \frac{p}{p}=1\]

\subsubsection{Poisson-Verteilung}
Eine ZV X heißt \textit{Poisson-verteilt} mit Parameter $\lambda >0$, falls
\[P(X=k)=\frac{\lambda^k}{k!}\cdot e^{-\lambda}, \hspace{15pt} k=0,1,2,\dots\]
Bez. X$\sim$Poi$(\lambda)$.
\newline Beachte
\[\sum_{k=0}^{\infty}\frac{\lambda^k}{k!}\cdot e^{-\lambda} = e^{-\lambda}\cdot \sum_{k=0}^{\infty}\frac{\lambda^k}{k!} = e^{-\lambda}\cdot e^\lambda = 1\]
über die Taylor-Reihe für $e^\lambda$.

\noindent Anwendung der Posison-Verteilung zu Approximation einer Binomialverteilung. 
\newline Sei X$\sim$Bin(n,p). Ist n \glqq groß\grqq\ und p \glqq klein\grqq\, etwa $n\geq50,p\leq\frac{1}{10}$ und $np\leq10$, dann
\[P(X=k)\approx P(Y=k), \text{wobei} Y~Poi(n,p)\]

\vspace{6pt}
\noindent\textbf{Bsp. 3.7:} 
Ein Callcenter erhält in jeder Sekunde mit W'keit 0,001 einen Anruf. 
Anrufe erfolgen unabhängig.

\noindent $X=$ Anzahl der Anrufe in einer Stunde
\newline $\sim$Bin(n,p) mit $n=3600$ und $p=0,001$
\newline $\Rightarrow$ Poisson-Approximation [$n\geq 50, p\leq\frac{1}{10}, np=2,6\leq10$]
\newline Approximiere Bin(n,p) durch Poi$(\lambda)$ mit $\lambda=np=3,6$.
\[P(X=k)=\frac{\lambda^k}{k!}\cdot e^{-\lambda}=P(Y=k) \hspace{15pt}\text{wobei } Y~Poi(\lambda)\]

\begin{center}
\begin{tabular}{c|c c c}
    k & 0 & 1 & 2 \\
    \hline
    $P(X=k)$ & .02727 & .09829 & .1770 \\
    $P(Y=k)$ & .02732 & .09837 & .1771 \\
\end{tabular}
\end{center}

\subsubsection{Hypergeometrische Verteilung}
Warenlieferung enthalten N Stücken, davon M defekt (N-M intakt)
\newline Es werden n Stücke ohne Zurücklegen zufällig gezogen, wobei $n\leq N$. Sei
\newline X = Anzahl der defekten Stücke in Stichprobe vom Umfang n
\newline Die W-funktion von X ist gegeben durch 
\begin{equation*}
f(x))= \begin{cases}
\frac{\binom{M}{x}\binom{N-M}{n-x}}{\binom{N}{n}}, & x=0,1,\dots,n\\
0, & \text{sonst}\\
\end{cases}
\end{equation*} 

\noindent Daher wird die Konvention
\[\binom{m}{k}=0\hspace{15pt}\text{ falls } k<0 \text{ oder } k>m\]
benutzt.
\newline Für $X\in\mathbb{N}_0$ gilt
\begin{align*}
f(X)<0 &\Leftrightarrow 0\leq x\leq M \text{ und } 0\leq n-x\leq N-M\\
&\Leftrightarrow x\in\{max(0,n-N+M),\dots,min(n,M)\}\\
\end{align*}

\noindent Eine ZV mit W-funktion f heißt \textit{hypergeometrisch verteilt} mit Parametern N,M, und n.
\newline Bez. X$\sim$Hyp(N,M,n).

\vspace{6pt}
\noindent\textbf{Bsp. 3.8:} Lotto 6 aus 49 
\newline Jemand hat die Zahlen 1,2,\dots,6 getippt. W'keit für 3 Richtige?

\vspace{4pt}
 \begin{tabular}{c c c}
    Warenlieferung & 49 Kugeln & $N=49$\\
    defekt & richtige Zahl $(1,\dots,6)$ & $M=6$\\
    intakt & falsche Zahl $(7,\dots,49)$ & $N-M=43$\\
\end{tabular}
\vspace{4pt}
\newline $n=6$ Kugeln werden ohne Zurücklegen gezogen.
\newline $X=$ Anzahl richig getippter Zahlen.
\newline W'keit für 3 Richtige:
\[P(X=3)=\frac{\binom{6}{3}\binom{49-6}{6-3}}{\binom{49}{6}}\approx0,01765\]

\subsection{Beispiele stetiger Verteilungen}

\subsubsection{Gleichverteilung}
\noindent\textbf{Gleichverteilung} auf $[a,b], \hspace{15pt} a<b$.
\newline Dichte
\begin{equation*}
    f(x)=\begin{cases}
    \frac{1}{b-a}, & x\in[a,b]\\
    0, & x\in\mathbb{R}\backslash[a,b]\\
    \end{cases}
\end{equation*}

\begin{tikzpicture}
    \draw[step=1cm,gray,very thin] (0,0) grid (4,4);
    \draw[thick,->] (0,0) -- (4,0) node[anchor=north west] {x};
    \draw[thick,->] (0,0) -- (0,4) node[anchor=south east] {$f(x)$};
    \draw(1,1pt) -- (1,-1pt) node[anchor=north] {$a$};
    \draw(3,1pt) -- (3,-1pt) node[anchor=north] {$b$};
    \draw(1pt,2) -- (-1pt,2) node[anchor=east] {$\frac{1}{b-a}$};
    \draw[red,thick] (0,0) -- (1,0);
    \draw[red,thick] (1,2) -- (3,2);
    \draw[red,thick] (3,0) -- (4,0);
\end{tikzpicture}

\noindent VF
\begin{equation*}
F(x)=\int_{-\infty}^{x}f(u)du = \begin{cases}
0, & x<a\\
\int_{a}^{x}\frac{1}{b-a}du=\frac{x-a}{b-a}, & a\leq x\leq b\\
1, & x>b\\
\end{cases}
\end{equation*}

\begin{tikzpicture}
    \draw[step=1cm,gray,very thin] (0,0) grid (4,4);
    \draw[thick,->] (0,0) -- (4,0) node[anchor=north west] {x};
    \draw[thick,->] (0,0) -- (0,4) node[anchor=south east] {$F(x)$};
    \draw(1,1pt) -- (1,-1pt) node[anchor=north] {$a$};
    \draw(3,1pt) -- (3,-1pt) node[anchor=north] {$b$};
    \draw(1pt,3) -- (-1pt,3) node[anchor=east] {$1$};
    \draw[red,thick] (0,0) -- (1,0);
    \draw[red,thick] (1,0) -- (3,3);
    \draw[red,thick] (3,3) -- (4,3);
\end{tikzpicture}



\vspace{6pt}
\noindent\textbf{Bsp. 3.9:} Ist X$\sim$Uni(0,1), dann gilt für $0\leq c \leq d \leq 1$
\[P(c\leq X\leq d) = \int_{c}^{d}1dx=d-c\]
und für $c<0\leq d\leq 1$ gilt
\[P(c\leq X\leq d)=\int_{c}^{0}0dx + \int_{0}^{d}1dx=d\]

\subsubsection{Exponentialverteilung}
\noindent mit Parameter $\lambda>0$
\newline Bez. X$\sim$Exp($\lambda$)
\newline Diese Verteilung wird oft beutzt zur Modellierung von Wartezeiten oder Lebensdauern

\vspace{6pt}
\noindent Dichte
\begin{equation*}
f(x)=\begin{cases}
\lambda e^{-\lambda x} &, X\geq0\\
0 &, x<0\\
\end{cases}
\end{equation*}
\newline VF
\begin{equation*}
F(x)=\int_{-\infty}^{x}f(u)du = \begin{cases}
\int_{0}^{x}\lambda e^{-\lambda u}du = -e^{-\lambda u} &, x\geq0\\
0 &, x<0\\
\end{cases}
\end{equation*}

\begin{tikzpicture}
    \draw[step=1cm,gray,very thin] (0,0) grid (4,4);
    \draw[thick,->] (0,0) -- (4,0) node[anchor=north west] {x};
    \draw[thick,->] (1,0) -- (1,4) node[anchor=south east] {};
    \draw(1+1pt,3) -- (1-1pt,3) node[anchor=east] {$1$};
    \draw[red,thick] (0,0) -- (1,0);
    \draw[red,thick] (1,0) arc (180:95:3cm);
\end{tikzpicture}

\[\int_{-\infty}^{\infty}f(u)du=\int_{0}^{\infty}\lambda e^{-\lambda x}=[e^{-\lambda x}]_0^\infty=1\]

\subsubsection{Normalverteilung} (oder Gaußverteilung)
\[N(\mu, \sigma^2),\mu\in\mathbb{R},\sigma^2>0\]
\[f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(x-\mu)^2}{2\sigma^2})\hspace{25pt}, x\in\mathbb{R}\]
\newline$\mu$: Mittelwert/Erwartungswert einer Zufallsvariable mit obiger Dichte
\newline$\sigma^2$: Varianz einer Zufallsvariable mit obiger Dichten
\newline $N(0,1)$ heißt \textit{Standardnormalverteilung}

\vspace{4pt}
\noindent Dichte 
\[\phi(X)=\frac{1}{\sqrt{2\pi}}\cdot exp(-\frac{X^2}{2})\]
\newline VF
\[\Phi(X)=\int_{-\infty}^{x}\phi(u)du\]
\newline $\rightarrow$ Abgeschlossen bezüglich affiner Transformation.
\newline Beweis:
\newline Sei X$\sim N(0,1)$, seien $a,b\in\mathbb{R},b\neq0$
\newline $\Rightarrow Y:=a+bX\sim N(a+b\mu,b^2\sigma^2)$
\newline Nachweis für den Fall $b>0$ (Fall $b<0$ analog)
\newline $P(Y\geq y) =$ VF der transformierten Zufallszahl
\begin{align*}
P(Y\geq y) &= P(a+bX\leq y) = P(X\leq\frac{y-a}{b})\\
&= F_x(\frac{y-a}{b}), y\in\mathbb{R}\\
\Rightarrow \frac{d}{dy}P(Y\geq y) &= F_x'(\frac{y-a}{b})\frac{1}{b} = f(\frac{y-a}{b})\frac{1}{b}\\
&= \frac{1}{\sqrt{2\pi\sigma^2}}\cdot exp(-\frac{(\frac{y-a}{b}-\mu)^2}{2\sigma^2})\frac{1}{b}\\
&= \frac{1}{\sqrt{2\pi(b\sigma)^2}}\cdot exp(-\frac{[y-(a+b\cdot\mu)]^2}{2(b\sigma)^2})
\end{align*}
\noindent $\Rightarrow$ Dichte von Y ist die Dichte der $N(a+b\mu,b^2\sigma^2)$-Verteilung
\newline Insb. gilt:
\newline Falls $X\sim N(0,1)$, dann $a+bX\sim N(a,b^2)$
\newline Falls $X\sim N(\mu,\sigma^2)$, dann $\frac{X-\mu}{\sigma}\sim N(0,1)$ 
\newline\indent und $P(X\leq x) = P(\frac{X-\mu}{\sigma}\leq\frac{x-\mu}{\sigma}) = \Phi(\frac{x-\mu}{\sigma})$, $x\in\mathbb{R}$
\newline Dabei ist $\Phi$ die VF einer $N(0,1)$-verteilten ZV. $\frac{X-\mu}{\sigma}$ ist die zu X gehörende standardisierte ZV.
Durch Standardisierung kann eine beliebige Normverteilung als Standard-Normalverteilung dargestellt werden.

\section{Erwartungswert, Varianz und Kovarianz}

\subsection{Erwartungswert einer ZV}
Sei X eine diskrete ZV, und seien $x_1,x_2,\dots$ die verschiedenen möglichen Werte von f, sei f die W'funktion von X. 
\newline Der \textit{Erwartungswert} (EW) von X wird definiert durch
\[E(X)=\sum_{i}x_if(x_i)=\sum_{i}x_iP(X=x_i) \hspace{15pt}(\text{gewichtetes Mittel der }x_i)\]

\vspace{6pt}
\noindent Hier wird angenommen, dass $E(X)=\sum_{i}x_if(x_i)<\infty$ ist. Dies stellt sicher, dass der rruwer wohldefiniert ist und nicht von der Summationsrechenfolge abhängt. 
F+r nichtnegative diskrete ZV X ist $E(X)$ immer nichtdefiniert, aber eventuell $=\infty$.

\vspace{6pt}
\noindent\textbf{Bsp. 4.1:}
\begin{itemize}
\item[(a)] Sei $\Omega=\{1,\dots,n\}, P(A)=\frac{|A|}{|\Omega|}$ für $A\subset\Omega$ und $a_\omega=X(\omega),\omega\in\mathbb{R}=4$.
\newline \begin{align*}
\Rightarrow E(X) &= \sum_{x\in\{a_1,\dots,a_n\}}xP(X=x)\\
&= \sum_{x\in\{a_1,\dots,a_n\}}x\frac{|\{\omega\in\Omega:X(\omega)=x\}|}{n}\\
&= \frac{1}{n}\cdot\sum_{i=1}^{n}a_i\\
\end{align*}
\item[(b)] Ist $P(X=a)=1, a\in\mathbb{R}$, dann ist 
\[E(X)=a\cdot P(X=a)=a\]
\item[(c)] Sei $X\sim Ber(p), X=I_A, P(A)=p$
\begin{align*}
\Rightarrow E(X)=E(I_A) &= 0\cdot P(I_A=0)+1\cdot P(I_A=1)\\
&= 1\cdot P(A)=p\\
\end{align*}
\item[(d)] Für $X\sim Poi(\lambda)$ gilt: 
\begin{align*}
E(X) &= \sum_{k=0}^{\infty}kP(X=k) = \sum_{k=1}^{\infty}k\frac{\lambda^k}{k!}e^{-\lambda}\\
&= e^{-\lambda}\sum_{k=1}^{\infty}\frac{\lambda^k}{(k-1)!} \hspace{10pt} \jekmo \hspace{10pt} e^{-\lambda}\sum_{j=0}^{\infty}\frac{\lambda^{j+1}}{j!}\\
&= e^{-\lambda}\lambda\underbrace{\sum_{j=0}^{\infty}\frac{\lambda^j}{j!}}_{= e^\lambda}=\lambda
\end{align*}
\end{itemize}

\subsubsection{Satz 4.1:}
Ist X eine ZV, die nur Werte in $\{0,1,2,\dots\}$ annimt, dann gilt
\[E(X)=\sum_{n=0}^{\infty}P(X>n)=\sum_{n=1}^{\infty}P(X\geq n)\]

\noindent\textbf{Bew.:}
\begin{align*}
E(X) = \sum_{n=1}^{\infty}nP(X=n) =& P(X=1)\\
& + P(X=2) + P(X=2)\\
& +P(X=3) + P(X=3) + P(X=3)\\
& + \dots\\
=& P(X>0) + P(X>1) + P(X>2) + \dots\\
=& P(X\geq 1) + P(X\geq 2) + P(X\geq 3) + \dots\\
\end{align*}

\vspace{6pt}
\noindent\textbf{Bsp. 4.2:} Ist $X\sim$Geo($p$), dann
\[P(X>n)=(1-p)^n, n=0,1,\dots\]
und
\[E(X)=\sum_{n=0}^{\infty}P(X>n)=\sum_{n=0}^{\infty}(1-p)^n=\frac{1}{p}\] 

\vspace{6pt}
\noindent Sei X eine stetige Zufallsvariable mit Dichte f. Dann ist der Erwartungswert von X definiert durch
\[E(X)=\int_{-\infty}^{\infty}xf(X)dx\]
Hier wird angenommenn, dass $\int_{-\infty}^{\infty}xf(X)dx<\infty$ ist. 
\newline Für nichtnegative stetige ZV X ist $E(X)$ immer wohldefiniert, aber evtl. $\infty$. 

\vspace{6pt}
\noindent\textbf{Bsp. 4.3:}
\begin{itemize}
    \item[(a)] Sei $X\sim$Uni($a,b$)
    \begin{align*}
    \Rightarrow E(X) &= \int_{a}^{b}x\frac{1}{b-a}dx = \frac{1}{b-a}\frac{x^2}{2}|_a^b = \frac{1}{b-a}\frac{b^-a^2}{2}\\
    &= \frac{a+b}{2} \text{Mittelpunkt von }[a,b]
    \end{align*}
    \item[(b)] Sei $X\sim$Exp($\lambda$)
    \begin{align}
    E(X) &= \int_{0}^{\infty}x\lambda e^{-\lambda x}dx\\
    &= [-xe^{-\lambda x}]_0^\infty - \int_{0}^{\infty}e^{-\lambda x}\\
    &= 0 + [-\frac{1}{\lambda}e^{-\lambda x}]_{0}^{\infty}=\frac{1}{\lambda}\\
    E(X) &= \frac{1}{\lambda}\\
    \end{align}
\end{itemize}

\subsubsection{Satz 4.2:}
Seien $X,Y,X_1,\dots,X_n$ beliebige ZV (diskret oder stetig).
\begin{itemize}
    \item[(a)]
        \begin{align*}
        E(aX+bY) &= aE(X)+bE(Y) \text{für alle $a,b\in\mathbb{R}$ (Linearität)}\\
        E(a+bX) &= a+bE(X) \text{für alle $a,b\in\mathbb{R}$}\\
        E(\sum_{i=1}^{n}a_iX_i)=\sum_{i=1}^{n}a_i\cdot E(X_i)\\
        \end{align*}
    \item[(b)] Falls $X_1,\dots,X_n$ unabhängig sind, dann gilt
        \[E(X_1,\dots,X_n)=E(X_1)\cdot\dots\cdot E(X_n)\]
    \item[(c)] Falls $X$ eine nichtnegative ZV ist, dann gil
        \[E(X)\geq0 \text{mit Gleichheit genau dann, wenn } P(X=0)=1\]
    \item[(d)] Falls $P(X\leq Y)=1$, dann $E(X)\leq E(Y)$ (Monotonie)
\end{itemize}

\vspace{6pt}
\noindent\textbf{Bsp. 4.4:}
\begin{itemize}
\item[(a)]$X\sim$ Bin($n,p$)
\newline Benutze Darstellung $X=X_1+\dots+X_n, X_i\sim$ Ber($p$)
\[\Rightarrow E(X) \hspace{15pt} \sfotwoa \hspace{15pt} E(X_1)+\dots+E(X_n)\]
\item[(b)] $X\sim$ N($\rho,\sigma^2$)
\newline $X$ hat dieselbe Verteilung wie $\mu + \sigma Y$, wobei $Y\sim$ N($0,1$)
\begin{align*}
\Rightarrow E(X) &= E(\mu + \sigma Y) = \mu + \sigma E(Y)\\
E(Y) &= \int_{-\infty}^{\infty}y\frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}dy=0, \text{ da } ye^{-\frac{y^2}{2}} \text{ ungerade.}\\
\end{align*}
\end{itemize}

\subsubsection{Satz 4.3:} Seien X eine ZV und $g:\mathbb{R}\rightarrow\mathbb{R}$ eine Funktion. Dann gilt
\begin{equation*}
E[g(X)] = \begin{cases}
\sum_{i}^{}g(X_i)P(X=x_i) & \text{falls X diskret ist mit möglichen Werten  } x_1,x_2,\dots\\
\int_{-\infty}^{\infty}g(x)f(x)dx & \text{falls X stetig ist und Dichte f hat}\\
\end{cases}
\end{equation*}

\vspace{6pt}
\noindent\textbf{Bsp. 4.5:} X$\sim$N(0,1)
\begin{align*}
E(X^2) &\hspace{30pt}\sfothree\hspace{22pt} \int_{-\infty}^{\infty}x^2\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}(-x)(-xe^{-\frac{-x^2}{2}})dx\\
&= \frac{1}{\sqrt{2\pi}}([-xe^{-\frac{-x^2}{2}}]_{-\infty}^[infty]+\int_{-\infty}^{\infty}e^{-\frac{-x^2}{2}}dx)\\
&= \int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{-x^2}{2}}dx
\end{align*} %that bottom right corner check thing
\newline partielle Integration
\begin{align*}
u(x)=-x & v(x) = e^{-\frac{-x^2}{2}}\\
u'(x)=-1 & v'(x) = -xe^{-\frac{-x^2}{2}}\\
\end{align*}

\subsection{Varianz einer ZV}
Die \textit{Varianz} einer ZV X ist det. durch 
\[Var(x):=E[(X-E(X))^2]\]
und die \textit{Standardabweichung} von X ist 
\[\sigma(X):=\sqrt{Var(X)}\]
$Var(X)$ und $\sigma(X)$ sind Maße für die Streuung der Verteilung von X um E(X)

\subsubsection{Satz 4.4:} 
Sei X eine ZV. Dann gilt
\begin{itemize}
\item[(a)] $Var(X)=E(X^2)-[E(X)]^2$
\item[(b)] Ist P(X=a)=1 für eine Konstante $A\in\mathbb{R}$, dann $Var(X)=c$.
\newline Ist $Var(X)=c$, dann folgt $P(X=E[X])=1$, d.h. mit W'keit 1 ist X gleich der Konstanten E(X).
\item[(c)] $Var(a+bX)=Var(bX)=b^2Var(X)$ für alle $a,b\in\mathbb{R}$
\item[(d)] Sei $Var(X)\in(0,\infty)$. Für die standardisierte ZV 
\[X* = \frac{X-E(X)}{\sqrt{Var(X)}}\]
gilt $E(X*)=0, Var(X*)=1$.
\item[(e)] Sind $X_1,\dots,X_n$ unabhängige ZV, dann gilt
\[Var(\sum_{i=1}^{n}X_i)=\sum_{i=1}^{n}Var(X_i)\]
\end{itemize}

\noindent Beweis nur für (a):
\begin{align*}
Var(X) &= E[(X-E(X))^2]\\
&= E[X^2-2XE(X)+[E(X)]^2]\\
&= E(X^2)-2E[XE(X)]+E[[E(X)]^2]\\
&= E(X^2) - 2E(X)E(X) + (E(X))^2\\
&= E(X^2)-[E(X)]^2\\
\end{align*}

\vspace{6pt}
\noindent\textbf{Bsp. 4.6:} (a) X$\sim$Ber(p)
\newline Da $X=0$ oder $X=1$, folgt $X^2=X$. 
\newline $\Rightarrow Var(X) \hspace{12pt}\sfofoa\hspace{12pt} E(X^2)-[E(X)]^2=E(X)-[E(X)]^2 \hspace{12pt}\bspfoonc\hspace{12pt} p-p^2 = p(1-p)$
\vspace{4pt}
\noindent (b) X$\sim$Bin(n,p)
\newline $\Rightarrow X=X_1+\dots+X_n,$ $X_i$ unabhängig, $X_i\sim$Ber(p)
\newline $\Rightarrow Var(X)= Var(X_1+\dots+X_n) \hspace{12pt}\sfofoe\hspace{12pt} Var(X_1)+\dots+Var(X_n)=np(1-p)$
\vspace{4pt}
\noindent (b) X$\sim N(\mu,\sigma^2)$, Var(X)=?
\newline Sei $Y\sim N(0,1)$
\newline $\Rightarrow$ X hat dieselbe Verteilung wie $\mu+\sigma Y$
\newline $\Rightarrow Var(X)= Var(\mu + \sigma Y) \hspace{12pt}\sfofoc\hspace{12pt} \sigma^2Var(Y) = \sigma^2(E(Y^2)-(E(Y))^2)=\sigma^2$

\subsection{Kovarianz und Korrelation}
Sind X und Y ZV mit EW $\mu_x=E(X),\mu_y=E(Y)\in\mathbb{R}$.
\newline Die \textit{Kovarianz} von X und Y ist def. durch
\[Cov(X,Y):=E[(X-\mu_x)(Y-\mu_y)]\]
\newline Der \textit{Korrelationskoeffizient} von X und Y ist def. durch 
\[\rho(X,Y):=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}\]
falls $Var(X),Var(Y)\in(0,\infty)$
\newline $\rho(X,Y)$ ist ein Maß für den linearen Zusammenhang zwischen X und Y.
\newline Es gilt 
\[-1\leq\rho(X,Y)\leq1\]
$\rho(X,Y)=1 \Leftrightarrow$ es ex. $A\in\mathbb{R},b>0$ mit $P(Y=a+bX)=1$.
$\rho(X,Y)=-1 \Leftrightarrow$ ex ex. $A\in\mathbb{R},b<0$ $P(Y=a+bX)=1$.

\vspace{6pt}
\noindent X und Y heißen \textit{unkorreliert}, falls $\rho(X,Y)=0$.
\newline Es gilt 
\begin{align*}
Cov(X,Y) &= E(XY-\mu_xY-X\mu_y+\mu_x\mu_y)\\
&= 
\end{align*}
$\Rightarrow$ Sind X und Y unabhängig, dann sind X und Y unkorreliert. Umkehrung gilt nicht.

\vspace{6pt}
\noindent\textbf{Bsp. 4.7:} Sei $P(X=-1)=P(X=0)=P(X=1)=\frac{1}{3}, Y:=X^2$
\newline $\Rightarrow E(X)=0,E(XY)=E(X^3)=0$, also
\newline $Cov(X,Y)=E(XY)-E(X)E(Y)=0$
\newline X und Y sind unkorreliert, aber
\newline $P(X=0|Y=0)=1 \neq P(X=0)$
\newline X und Y sind nicht unabhängig.

\subsubsection{Satz 4.5:} Seien $X,Y,X_1,\dots,X_n,Y_1,\dots,Y_n$ ZV
\begin{itemize}
    \item[(a)] $Cov(X,Y) = Cov(Y,X)$
    \item[(b)] $Cov(X,X) = Var(X)$
    \item[(c)] $Cov(a+bX,c+dY) = bdCov(X,Y)$ für alle $a,b,c,d\in\mathbb{R}$
    \item[(d)] $Cov(\sum_{i=1}^{m}a_iX_i,\sum_{j=1}^{n}b_jY_j) = \sum_{i=1}^{m}\sum_{j=1}^{n}a_ib_jCov(X_i,Y_j)$ 
        $\hspace{1pt} \forall a_1,\dots,a_m,b_1,\dots,b_n\in\mathbb{R}$
    \item[(e)] $Var(\sum_{i=1}^{m}X_i)=\sum_{i=1}^{m}Var(X_i)+2\sum_{i\neq j}Cov(X_i,X_j)$
\end{itemize} 

\noindent\textbf{Bew. (c):}
\begin{align*}
& Cov(a+bX,c+dY)\\
=&E[(a+bX-E(a+bX))(c+dY-E(c+dY))]\\
=&E[b(X-E(X))d(Y-E(Y))]\\
=&b\cdot d\cdot Cov(X,Y)\\
\end{align*}

\noindent\textbf{Bew. (d):}
\begin{align*}
    & Cov(\sum_{i}a_iX_i,\sum_{j}b_jX_j)\\
    =& E(\sum_{i}\sum_{j}a_ib_iX_iY_i) - (\sum_{i}a_iE(X_i))(\sum_{j}b_jE(Y_j))\\
    =& \sum_{i}\sum_{j}a_ib_jE(X_iY_j) - a_ib_jE(X_i)E(Y_j)\\
    \underset{\mathclap{\scriptscriptstyle (a)}}{=}& \sum_{i}\sum_{j}a_ib_jCov(X_i,Y_j)\\
\end{align*}

\noindent\textbf{Bew. (e):}
\begin{align*}
    Var(\sum_{i=1}^{m}X_i) \underset{\mathclap{\scriptscriptstyle (b)}}{=}& Cov(\sum_{i=1}^{m}X_i,\sum_{j=1}^{m}X_j)\\
    \underset{\mathclap{\scriptscriptstyle (d)}}{=}& \sum_{j=1}^{m} Cov(X_i,X_j) \\
    =& \sum_{i=1}^{m} Cov(X_i,X,i) + \sum_{i\neq j} Cov(X_i,X_j) \\
    =& \sum_{i=1}^{m} Var(X_i)+2\sum_{i<j}Cov(X_i,X_j)\\
\end{align*}

\subsection{Ungleichungen}

\subsubsection{Satz 4.6: Markov-Ungleichung}
Sei X eine nicht-negative ZV. Dann gilt 
\[P(X\geq t) \leq \frac{E(X)}{t} \hspace{20pt}\forall t>0\]

\noindent\textbf{Beweis:}
Definiere eine Bernoulli-ZV durch
\begin{align*}
Y(\omega) &:= \begin{cases}
1 \text{ falls } X(\omega) \geq t\\
0 \text{ falls } X(\omega) < t\\
\end{cases}\\
\Rightarrow Y(\omega) &\leq \frac{X(\omega)}{t} \hspace{10pt}\forall \omega\\
\end{align*}
\begin{itemize}
\item Für $X(\omega) < t$ gilt $Y(\omega)=0\leq\frac{X(\omega)}{t}$
\item Für $X(\omega) \geq t$ gilt $Y(\omega)=1\leq\frac{X(\omega)}{t}$
\end{itemize}
\[\Rightarrow P(X\geq t) \hspace{5pt}\underset{\mathclap{\scriptscriptstyle Bsp. 4.1}}{=}\hspace{5pt} E(Y) \hspace{5pt}\underset{\mathclap{\scriptscriptstyle Satz 4.2(d)}}{\leq}\hspace{5pt} E(\frac{X}{t}) \hspace{10pt}\underset{\mathclap{\scriptscriptstyle Satz 4.2(a)}}{=}\hspace{10pt} \frac{1}{t}E(X)\]

\vspace{6pt}
\noindent\textbf{Bsp. 4.8:}
Sei $X\sim$Poi($1$) und $t>1$.
\[P(X\geq t) \hspace{5pt}\underset{\mathclap{\scriptscriptstyle Markov}}{\leq}\hspace{5pt} \frac{E(X)}{t} \hspace{10pt}\underset{\mathclap{\scriptscriptstyle Bsp. 4.1(d)}}{=}\hspace{10pt} \frac{1}{t}\]
Die obere Schranke lässt sich verbessern. Für jedes $u>0$ gilt 
\[\underset{\mathclap{\scriptscriptstyle (monotone\hspace{3pt}Transf.)}}{P(X\geq t)} = P(e^{uX}\geq e^{ut}) \hspace{5pt}\underset{\mathclap{\scriptscriptstyle Markov}}{\leq}\hspace{5pt} = \frac{E(e^{uX})}{e^{ut}}\]
und 
\[E(e^{uX}) \hspace{5pt}\underset{\mathclap{\scriptscriptstyle Satz 4.3}}{=}\hspace{5pt} e^{-1}\sum_{k=0}^{\infty}\frac{(e^u)^k}{k!} = e^{-1}\text{exp}(e^u-1)\]
\[\Rightarrow P(X\geq t)\leq \text{exp}(e^u-1-ut) \hspace{25pt}\forall u>0\]
$e^u-1-ut$ wird minimal für $u=log(t)$ und für dieses u ergibt sich 
\[P(X\geq t)\leq\text{exp}(t-1-t\cdot log(t)) = (\frac{e}{t}^t\cdot e^{-1})\]
z.B. $P(X\geq5)\leq 0,01747, P(X\geq10)\leq8,103\cdot10^{-7}$

\subsubsection{Satz 4.7: Tschebyscheff-Ungleichung}
Sei X eine ZV, $\mu:=E(X)\in\mathbb{R}$ und $\sigma^2:=Var(X).$ Dann gilt
\[P(|X-\mu|\geq t)\leq \frac{\sigma^2}{t^2} \forall t>0\]
Ist  $\sigma=\sqrt{Var(X)}\in (0,\infty)$, dann gilt
\[P(|X-\mu|\geq k\sigma)\leq\frac{1}{k^2}\forall k>0\]

\noindent\textbf{Beweis:}
\[P(|X-\mu|\geq t) = P((>-\mu)^2\geq t^2) \hspace{5pt}\underset{\mathclap{\scriptscriptstyle Markov}}{\leq}\hspace{5pt} \frac{E[(X-\mu)^2]}{t^2} = \frac{\sigma^2}{t^2}\]
Mit $t=k\sigma$ ergibt sich die andere Behauptung.

\vspace{6pt}
\noindent\textbf{Bsp. 4.9:}
n = 10.000 Münzwürfe
X = Anzahl der Würfe, in denen Zahl fällt $\sim$Bin($n,\frac{1}{2}$)
\begin{align*}
P(4.750 < X < 5.250) =& P(|X-5000|<250)\\
=& 1 - P(|X-E(X)|\geq 250) \\
\underset{\mathclap{\scriptscriptstyle Tschebyscheff}}{\geq}& 1- \frac{Var(X)}{250^2} = 1-\frac{\frac{n}{4}}{250^2} \\
=& 0,96 \\
\end{align*}

\vspace{6pt}
\noindent\textbf{Bsp. 4.10:}
Sei x eine ZV, $mu=E(X), \sigma=\sqrt{Var(X)} \in (0,\infty)$
\newline Für den $k\sigma$-Bereich von X 
\[\mu - k\sigma, \mu + k\sigma\]
gilt
\[P(X\notin[\mu - k\sigma, \mu + k\sigma])\leq\frac{1}{k^2}\]
und
\[P(X\in[\mu - k\sigma, \mu + k\sigma])\geq 1-\frac{1}{k^2}, k=2,3,\dots\]
W'keit, dass eine ZV ... das k-fache ihrer Standardabweichung von ihrem EW ... mind. $1-\frac{1}{k^2}$.

\vspace{6pt}
\noindent Dies gilt für große Verteilungen .
\newline Zum Vergleich: Ist $X\sim$N($\mu,\sigma^2$), dann
\begin{align*}
P(X\in[\mu - k\sigma, \mu + k\sigma]) &= P(-k \leq \frac{X-\mu}{\sigma} \leq k) = \Phi(k) - \Phi(-k)\\
\text{(Tutorium A40) }&= 2\Phi(k)-1\\
&\approx \begin{cases}
0,6827, k=1\\
0,9545, k=2\\
0,9973, k=3\\
\end{cases}
\end{align*}

\section{Grenzwertsätze}
Seien $X_1,X_2,\dots$ unabhängige, identisch verteilte ZV, $\mu=E(X_1), \sigma = \sqrt{Var(X_1)}$
\newline Untersuche das Verhalten des Stichprobenmittel:
\[\bar{X_n} := \frac{X_1+\dots+X_n}{n}\]
Für große N 
\[E(\bar{X}_n) = \frac{1}{n}(E(X_1)+\dots+E(X_n))=\mu\]
\[Var(\bar{X}_n) = \frac{1}{n^2}(Var(X_1)+\dots+Var(X_n)) = \frac{\sigma^2}{n} \rightarrow 0 (\text{für }n\rightarrow\infty)\]
Intuitiv, Verteilung von $\bar{X}_n$ konzentriert sich für $n\rightarrow\infty$ in der Nähe von $\mu$, 
\[\bar{X}_n \text{ konvergiert gegen }\mu \text{ (Gesetz d. großen Zahlen)}\]
Zentraler Grenzwertsatz beschreibt die Verteilung der standardisierten ZV 
\[\sqrt{n}\frac{\bar{X_n}-\mu}{\sigma}\]
für $n\rightarrow\infty$

\subsection{Gesetz der großen Zahlen}
Wichtiger Spezialfall: $X_n\overset{\mathclap{\scriptscriptstyle P}}{\rightarrow} a$ für eine Konstante $a$. 
\newline d.h. 
\[\underset{\mathclap{\scriptscriptstyle n\rightarrow\infty}}{\text{lim}} P(a-\epsilon < X_n < a + \epsilon) = 1 \forall\epsilon>0\]

\vspace{6pt}
\noindent\textbf{Bsp. 5.1} 
Seien $U_1,U_2,\dots$ unabhängig, $U_i\sim U_{ni}(0,1)$
\[X_n := min\{U_1,\dots,U_n\}, n\in\mathbb{N}\]
Beh. $X_n\overset{\mathclap{\scriptscriptstyle P}}{\rightarrow} 0$
\newline Für jedes $\varepsilon>0$ gilt 
\begin{equation*}
P(|X_n-0|\geq \varepsilon) = P(X_n\geq\varepsilon)=\begin{cases}
(1-\varepsilon)^2, &\text{falls } 0<\varepsilon<1\\
0, &\text{falls } \varepsilon\geq1\\
\end{cases}
\end{equation*}

\vspace{6pt}
\noindent\textbf{Def. 5.1:} Eine Folge von ZV $X_1,X_2,\dots$ \textit{konvergiert stochastisch} gegen eine ZV X, geschrieben $X_n\overset{\mathclap{\scriptscriptstyle P}}{\rightarrow} X$, falls für jedes $\epsilon>0$
\[\underset{\mathclap{\scriptscriptstyle n\rightarrow\infty}}{\text{lim}} P(|X_n - X| \geq \varepsilon) = 0\]
gilt. 

\subsubsection{Satz 5.1: Schwaches Gesetz der großen Zahlen}
Seien $X_1,X_2,\dots$ unabhängige, identisch verteilte ZV mit $\mu=E(X_1)$ und $Var(X_1)<\infty$. 
\newline Dann folgt
\[\frac{1}{n}\sum_{i=1}^{n}X_i\overset{\mathclap{\scriptscriptstyle P}}{\rightarrow}\mu\]
\textbf{Bew.:}
\[E(\frac{1}{n}\sum_{i=1}^{n}X_i)=\frac{1}{n}\sum_{i=1}^{n}E(X_i)=\mu\]
Für jedes $\varepsilon>0$ gilt 
\begin{align*}
P(|\frac{1}{n}\sum_{i=1}^{n}X_i-\mu|\geq\varepsilon)\\
\text{(Tschebyscheff)} &\leq \frac{Var(\frac{1}{n}\sum_{i=1}^{n}X_i)}{\varepsilon^2} = \underset{\mathclap{\scriptscriptstyle \text{Satz 4.4}}}{=}\frac{1}{n^2\varepsilon^2} = \sum_{i=1}^{n}Var(X_i)\\
&= \frac{Var(X_1)}{n\varepsilon^2} \rightarrow 0 (n\rightarrow\infty)\\
\end{align*}

\vspace{6pt}
\noindent\textbf{Bsp. 5.2:}
Seien $X_1,X_2,\dots$ unabhängig, identisch verteilte ZV, $X_i\sim$Ber($p$)
\newline$\rightarrow E(X_1) = p$ und nach Satz 5.1 gilt
\[\frac{1}{n}\sum_{i=1}^{n}X_i\overset{\mathclap{\scriptscriptstyle P}}{\rightarrow}p\]
relative Häufigkeit der Erfolge in den ersten n Experimenten $\rightarrow$ Erfolgsw'keit

\vspace{6pt}
\noindent\textbf{Bsp. 5.3:}
Seien $X_1,X_2,\dots$ unabhängige, identisch verteilte ZV, sei $A\in\mathbb{R}$
Wende Satz 5.1 an auf Bernoulli-ZV
\begin{equation*}
Y_i(\omega)=\begin{cases}
1 &\text{falls }X_i(\omega)\in a \\
0 &\text{falls }X_i(\omega)\notin a\\
\end{cases}
\end{equation*}
\[\Rightarrow \frac{1}{n}\sum_{i=1}^{n}Y_i\overset{\mathclap{\scriptscriptstyle P}}{\rightarrow} E(Y_1) = P(X_1\in A)\]
relative Häufigkeit von \glqq$X_i\in A$\grqq in den ersten n Experimenten $\rightarrow$ W'keit von \glqq$X_i\in A$\grqq

\subsubsection{Satz 5.2: Starkes Gesetz der großen Zahlen}
Seien $X_1,X_2,\dots$ unabhängige, identisch verteilte ZV mit $\mu\in E(X_1)\in\mathbb{R}$. Dann gilt
\[P(\{\omega\in\Omega:\overset{\mathclap{\scriptscriptstyle \text{lim}}}{n\rightarrow\infty}\frac{1}{n}(X_1(\omega)+\dots+X_n(\omega)=\mu)\})=1\]

\subsection{Zentraler Grenzwertsatz}
\subsubsection{Satz 5.2 Zentraler Grenzwertsatz ZGS}
Seien $X_1,X_2,\dots$ unabhängige, identisch verteilte ZV. Sei
\begin{align*}
\sigma^2 &= Var(X_1)\in(0,\infty)\\
\mu &= E(X_1)\\ 
S_n &= X_1+\dots+X_n\\
S_n* &= \frac{S_n-E(S_n)}{\sqrt{Var(S_n)}} = \frac{S_n-n\mu}{\sqrt{n\sigma^2}}
\end{align*}



Dann gilt 
\[\overset{\mathclap{\scriptscriptstyle \text{lim}}}{n\rightarrow\infty}P(a\leq S_n*\leq b) = \frac{1}{\sqrt{2\pi}}\int_{a}^{b}e^\frac{-x^2}{2}dx = \Phi(b) - \Phi(a), -\infty\leq a\leq b\leq\infty\]
wobei $Phi(y)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{y}e^\frac{-x^2}{2}dx = $ VF einer N(0,1)-verteilten ZV ($\Phi(-\infty)=0,\Phi(\infty)=1$)

\vspace{6pt}
\noindent\textbf{Bem. 5.1:}
Unter den Voraussetzungen von Satz 5.3 gilt auch 
\[\overset{\mathclap{\scriptscriptstyle \text{lim}}}{n\rightarrow\infty} P(a<S_n*<b)=\overset{\mathclap{\scriptscriptstyle \text{lim}}}{n\rightarrow\infty}P(a<S_n*\leq b) = \overset{\mathclap{\scriptscriptstyle \text{lim}}}{n\rightarrow\infty}P(a\leq S_n*<b) = \Phi(b) - \Phi(a)\]
für $-\infty\leq a<b\leq\infty$.

\vspace{6pt}
\noindent ZGS kann oft benutzt werden, um W'keiten, die mit Summen von unabhängigen, identisch verteilten ZV gebildet werden, zu approximieren. 

\vspace{6pt}
\noindent\textbf{Bsp. 5.4:} Normalverteilungsapproximation der Binomialverteilung
Seien $X_1,X_2,\dots$ unabhängig $\sim$Ber($p$), $0<p<1$
$\rightarrow S_n = X_1 + \dots + X_n \sim$Bin($n,p$), $E(S_n)=np$, $Var(S_n)=np(1-p)$
\[\rightarrow P(a\leq S_n^*\leq b)  =P(a\leq \frac{S_n-np}{\sqrt{np(1-p)}}\leq b) \approx \Phi(b) - \Phi(a)\]
Faustregel: Approximation ist anwendbar falls $np\geq5$ und $n(1-p)\geq5$. 

\vspace{6pt}
\noindent\textbf{Bsp. 5.5:}
Sei X$\sim$Bin($200,\frac{1}{20}$), $P(5\leq X\leq15)=$?
\begin{itemize}
\item[(a)] Exakte Lösung 
\[P(5\leq X\leq15)=\sum_{k=5}^{15}\binom{200}{k}(\frac{1}{20})^k(\frac{19}{20})^{200-k} = 0,9292\]
\item[(b)] Normalverteilungsapproximation
$X=S_n\sim$Bin($n,p$) mit $n=200$ und $p=\frac{1}{20}$
\newline Approximation $P(5\leq S_n\leq15)$ wie in Bsp. 5.4
\newline Bringe gesuchte W'keit auf die Form 
\[P(a\leq \frac{S_n-np}{\sqrt{np(1-p)}}\leq b) \]
wobei $np=10$ und $\sqrt{np(1-p)}=\sqrt{\frac{19}{2}}=3,0822$
\begin{align*}
P(5\leq S_n\leq15) &= P(\frac{5-np}{\sqrt{np(1-p)}}\leq\frac{S_n-np}{\sqrt{np(1-p)}}\leq\frac{15-np}{\sqrt{np(1-p)}})\\
&= P(-1,6222\leq\frac{S_n-np}{\sqrt{np(1-p)}}\leq1,6222)\\
(\text{Über ZGS}) &\approx \Phi(1,6222)-\Phi(-1,6222)\\
& \hspace{30pt} \Phi(-x) = 1-\Phi(x) \text{ für alle } x\in\mathbb{R} \text{ s. Tutorium A40}\\
&\Rightarrow P(5\leq S_n\leq15) \approx 2\Phi(1,6222) - 1 \approx 2\cdot0,9474-1\approx 0,8948\\
\end{align*}
\end{itemize}

\part{Schließende Statistik}

\section{Parameterschätzung}
Sei X eine ZV mit (ganz oder teilweise) unbekannter Verteilung (die Verteilung der Grundgesamtheit). 
\newline Gesucht: Information über die Verteilung von X. 
\newline Betrachte dazu \textit{(Zufalls-)Stichprobe} vom Umfang n: $(X_1,\dots,X_n)$ D.h. Stichprobenvariablen $(X_1,\dots,X_n)$ sind unabhängige ZV, die jeweils dieselbe Verteilung haben wie X. 
\newline $n$ unabhängige Wiederholungen von X. 
\newline Konstruiere mit Hilfe der $X_1,\dots,X_n$ Schätzer für interesierenden Parameter der Verteilung, z.B. $\vartheta=E(X)$, oder $\vartheta=Var(X)$, oder, falls bekannt ist, dass $X\sim$Exp($\lambda$), aber $\lambda$ unbekannt, $\vartheta=\lambda$. 

\subsection{Erwartungstreue und Konsistenz}
Sei ($X_1,\dots,X_n$) eine Stichprobe. Sei $T_\mathbb{R}^n\rightarrow\mathbb{R}^p$. Dann heißt die Zufallsvariable
\[T(X_1,\dots,X_n)\]
eine \textit{Stichprobenfunktion} oder \textit{Statistik}. Wird $T$ zum Schätzen eines Parameters $\vartheta\in\mathbb{R}^p$ verwendet, so nennt man $T(X_1,\dots,X_n)$ eine \textit{Schätzfunktion} für (das Schätzen von) $\vartheta$. 
\newline In Anwendungen: Erhebe Stichprobe, d.h. Beobachtungswerte $x_1,\dots,X_n\in\mathbb{R} =$ Realisationen von $X_1,\dots,X_n$. 
\newline $T(X_1,\dots,X_n)$ ist ein Schätz\underline{wert} (keine ZV) für $\vartheta$. 
\newline Im Folgenden gelte für 
\begin{align*}
\Theta &= \text{Menge der möglichen Parameterwerte}\\
\Theta &\in \mathbb{R} \text{eindimensional}
\end{align*}

\vspace{6pt}
\noindent\textbf{Def. 6.1:} 
Eine Schätzfunktion $\hat{\vartheta} = T(X_1,\dots,X_n)$ für $\vartheta\in\Theta\in\mathbb{R}$ heißt \textit{erwartungstreu} (e-treu) für $\vartheta$, falls
\[E(\hat{\vartheta})=\vartheta \hspace{15pt}\forall\vartheta\in\Theta\]
Der \textit{Bias} oder die \textit{Verzerrung} von $\hat{\vartheta}$ ist definiert durch
\[\text{Bias}(\hat{\vartheta})=E(\hat{\vartheta})-\vartheta\]

\vspace{6pt}
\noindent\textbf{Bsp. 6.1:} Schätze $\mu=E(X)$. 
\newline Für das Stichprobenmittel
\[\bar{X}_n=\frac{1}{n}\sum_{i=1}^{n}X_i\]
gilt
\[E(\bar{X}_n)=\frac{1}{n}\sum_{i=1}^{n}E(X_i)=\mu\]
So ist $X_n$ e-treue Schätzfunktion für $\mu=E(X)$. 

$\bar{X}_n$ hat unter allen e-treuen linearen Schätzfunktionen also Schätzfunktionen der Form
\[T(X_1,\dots,X_n)=\sum_{i=1}^{n}c_iX_i\hspace{15pt}\text{ mit }c_1,\dots,c_n\in\mathbb{R}\]
die kleinste Varianz, denn
\newline Sei $T(X_1,\dots,X_n)=\sum_{i=1}^{n}c_iX_i$ e-treu für $\mu$
\[\Rightarrow \mu=E[T(X_1,\dots,X_n)]=\sum_{i=1}^{n}c_iE(X_i)=\mu\sum_{i=1}^{n}c_i, \text{ also }\sum_{i=1}^{n}c_i=1\]
und mit $\sigma^2=Var(X_i)$ gilt 
\begin{align*}
Var(T(X_1,\dots,X_n)) &= \sum_{i=1}^{n}c_i^2Var(X_i)\\
&= \sigma^2\sum_{i=1}^{n}(c_i-\frac{1}{n}+\frac{1}{n})^2\\
&= \sigma^2[\sum_{i=1}^{n}(c_i-\frac{1}{n})^2+\frac{2}{n}\sum_{i=1}^{n}(c_i-\frac{1}{n})^2+\frac{n}{n^2}]\\
&\geq \frac{\sigma^2}{n}=Var(\bar{X}_n)\\
\end{align*}

\vspace{6pt}
\noindent\textbf{Bsp. 6.2:} Schätze $\sigma^2=Var(X)$
\[\tilde{S^2}=\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar{X}_n)^2\]
Ist $\tilde{S^2}$ e-treu für $\sigma^2$?
\begin{align*}
E(\tilde{S^2}) &= \frac{1}{n}E[(\sum_{i=1}^{n}X_i^2)-2\bar{X}_n(\sum_{i=1}^{n}X_i)+n\bar{X}_n^2]\\
&= \frac{1}{n}(\sum_{i=1}^{n}E(X_i^2)-nE(\bar{X}_n^2))
\end{align*}
\begin{align*}
E(X_i^2) &= Var(X_i)+[E(X_i)]^2=\sigma^2+\mu^2\\
E(\bar{X}_n^2) &= Var(\bar{X}_n)+[E(\bar{X}_n)]^2=\frac{\sigma^2}{n}+\mu^2\\
\end{align*}

\[\Rightarrow E(\tilde{S^2})=\frac{1}{n}(n(\sigma^2+\mu^2)-n(\frac{\sigma^2}{n}+\mu^2))=\frac{n-1}{n}\sigma^2\]

\noindent $\tilde{S^2}$ ist nicht e-treu für $\sigma^2$.
\[Bias(\tilde{S^2})=E(\tilde{S^2})-\sigma^2=-\frac{1}{n}\sigma^2\]

\noindent Aber: Die \textit{Stichprobenvarianz}
\[S_n^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X}_n)^2, n\geq2\]
ist e-treu für $\sigma^2=Var(X)$, denn $E(S_n^2)=E(\frac{n}{n-1}S^2)=\sigma^2$

\vspace{6pt}
\noindent\textbf{Def. 6.2:} Die \textit{mittlere quadratische Abweichung} (oder der mittlere quadratische Fehler) einer Schätzfunktion $\hat{\vartheta}$ für $\vartheta$ ist definiert durch
\[MSE(\hat{\vartheta}):=E[(\hat{\vartheta}-\theta)^2]\]

\vspace{6pt}
\noindent\textbf{Bem. 6.1:}
\begin{itemize}
\item[(a)] Ist $\hat{\vartheta}$ e-treu für $\theta$, dann gilt 
\[MSE(\hat{\vartheta}):=E[(\hat{vartheta}-\theta)^2]=Var(\hat{\vartheta})\]
\noindent In Bsp. 6.1
\[MSE(\bar{X}_n)=Var(\bar{X}_n)=\frac{\sigma^2}{n}\]
\item[(b)] Für bel. Schätzfunktionen $\hat{\vartheta}$ gilt
\[MSE(\hat{\vartheta})=[Bias(\hat{\vartheta})]^2+Var(\hat{\vartheta})\]
\noindent denn mit $t:=E(\hat{\vartheta})$ gilt
\begin{align*}
MSE(\hat{\vartheta}) &= E[(\hat{vartheta}-t+t-\theta)^2]\\
&= E[(\hat{\vartheta})^2]+2(t-\vartheta)E(\vartheta-t)+(t-\vartheta)^2\\
&= Var(\hat{\vartheta}) + [Bias(\hat{\vartheta})]^2\\
\end{align*}
\end{itemize}

\subsubsection{Konsistenz}
Seien $X_1,X_2,\dots$ unabh. identisch verteilte ZV. 
\newline Betrachte Folge von Schätzfunktionen $f\rightarrow$ der Schätzer eines Parameters:
\[T_1(X_1),T_2(X_1,X_2),T_2(X_1,X_2,X_3)\]
Für jedes n sei $T_n(X_1,\dots,X_n)$ Schätzfunktion basierend auf Stichprobe vom Umfang n. 
\newline Wird für $n\rightarrow\infty$ Schätzgenauigkeit bel. gut?

\vspace{6pt}
\noindent\textbf{Def. 6.3:}
Eine Folge von Schätzfunktionen für $\vartheta$:
\[T_1(X_1),T_2(X_1,X_2),\]
heißt \textit{konsistent} für den Parameter $\vartheta$, falls 
\[T_n(X_1,\dots,X_n) \overset{\mathclap{\scriptscriptstyle P}}{\rightarrow}\vartheta \hspace{15pt} (n\rightarrow\infty)\]

\vspace{6pt}
\noindent\textbf{Bsp. 6.3:}
Seien $X_1,X_2,\dots$ unabh., identisch verteilte ZV mit unbekanntem EW $\mu=E(X_i)$. 
Nach dem schwachen Gesetz der großen Zahlen gilt 
\[\bar{X}_n \overset{\mathclap{\scriptscriptstyle P}}{\rightarrow} \mu\]
Also ist die Folge $(\bar{X}_n)_{n=1}^\infty$ konsistent für $\mu$. 

\vspace{6pt}
\noindent\textbf{Bem. 6.2:} Seien $X_1,X_2,\dots$ unabh., identisch verteilte ZV, 
\[\hat{\vartheta}_n=T_n(X_1,\dots,X_n)\]
und $MSE(\hat{\vartheta}_n)\rightarrow0 \forall n\rightarrow\infty$. 
\newline $\Rightarrow$ Für jedes $\varepsilon>0$ gilt 
\[P(|\hat{\vartheta}_n-\vartheta|\geq\varepsilon)=P(|\hat{\vartheta}_n-\vartheta|^2\geq\varepsilon^2)\underset{\mathclap{\scriptscriptstyle Markov}}{\leq}\frac{E()}{\varepsilon^2}=\frac{MSE(\hat{\vartheta})}{\varepsilon^2}\rightarrow0\]
\[\Rightarrow\hat{\vartheta}_n\overset{\mathclap{\scriptscriptstyle P}}{\rightarrow}\vartheta \text{ und } (\hat{\vartheta}_n)_{n=1}^\infty \text{ ist konsistent.}\]
\noindent Mit Bem. 6.1(b) folgt:
\newline Falls $Bias(\hat{\vartheta}_n)\rightarrow0$ und $Var(\hat{\vartheta}_n)\rightarrow0$, dann ist $(\hat{\vartheta}_n)_{n=1}^\infty$ konsistent. 


\end{document}